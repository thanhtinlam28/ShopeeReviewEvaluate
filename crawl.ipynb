{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Thứ tự chạy: 1\r\n",
    "\r\n",
    "#Train model nhan dien cam xuc trong cau#\r\n",
    "\r\n",
    "# -*- coding: utf-8 -*-\r\n",
    "from __future__ import print_function\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "# from sklearn.neural_network import MLPClassifier\r\n",
    "# from sklearn.tree import DecisionTreeClassifier\r\n",
    "# from sklearn.naive_bayes import MultinomialNB\r\n",
    "# from sklearn.linear_model import SGDClassifier\r\n",
    "# from sklearn.neighbors import KNeighborsClassifier\r\n",
    "# from sklearn.ensemble import AdaBoostClassifier\r\n",
    "# from sklearn.ensemble import RandomForestClassifier\r\n",
    "import pandas as pd\r\n",
    "from pyvi import ViTokenizer\r\n",
    "import re\r\n",
    "import string\r\n",
    "import codecs\r\n",
    "\r\n",
    "#Từ điển tích cực, tiêu cực, phủ định\r\n",
    "path_nag = './sentiment/sentiment_dicts/nag.txt'\r\n",
    "path_pos = './sentiment/sentiment_dicts/pos.txt'\r\n",
    "path_not = './sentiment/sentiment_dicts/not.txt'\r\n",
    "\r\n",
    "with codecs.open(path_nag, 'r', encoding='UTF-8') as f:\r\n",
    "    nag = f.readlines()\r\n",
    "nag_list = [n.replace('\\n', '') for n in nag]\r\n",
    "\r\n",
    "with codecs.open(path_pos, 'r', encoding='UTF-8') as f:\r\n",
    "    pos = f.readlines()\r\n",
    "pos_list = [n.replace('\\n', '') for n in pos]\r\n",
    "with codecs.open(path_not, 'r', encoding='UTF-8') as f:\r\n",
    "    not_ = f.readlines()\r\n",
    "not_list = [n.replace('\\n', '') for n in not_]\r\n",
    "\r\n",
    "\r\n",
    "VN_CHARS_LOWER = u'ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđð'\r\n",
    "VN_CHARS_UPPER = u'ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸÐĐ'\r\n",
    "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\r\n",
    "def no_marks(s):\r\n",
    "    __INTAB = [ch for ch in VN_CHARS]\r\n",
    "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\r\n",
    "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\r\n",
    "    __r = re.compile(\"|\".join(__INTAB))\r\n",
    "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\r\n",
    "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\r\n",
    "    return result\r\n",
    "\r\n",
    "def normalize_text(text):\r\n",
    "\r\n",
    "    #Remove các ký tự kéo dài: vd: đẹppppppp\r\n",
    "    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\r\n",
    "\r\n",
    "    # Chuyển thành chữ thường\r\n",
    "    text = text.lower()\r\n",
    "\r\n",
    "    #Chuẩn hóa tiếng Việt, xử lý emoj, chuẩn hóa tiếng Anh, thuật ngữ\r\n",
    "    replace_list = {\r\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\r\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\r\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\r\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\r\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\r\n",
    "        #Quy các icon về 2 loại emoj: Tích cực hoặc tiêu cực\r\n",
    "        \"👹\": \"nagative\", \"👻\": \"positive\", \"💃\": \"positive\",'🤙': ' positive ', '👍': ' positive ',\r\n",
    "        \"💄\": \"positive\", \"💎\": \"positive\", \"💩\": \"positive\",\"😕\": \"nagative\", \"😱\": \"nagative\", \"😸\": \"positive\",\r\n",
    "        \"😾\": \"nagative\", \"🚫\": \"nagative\",  \"🤬\": \"nagative\",\"🧚\": \"positive\", \"🧡\": \"positive\",'🐶':' positive ',\r\n",
    "        '👎': ' nagative ', '😣': ' nagative ','✨': ' positive ', '❣': ' positive ','☀': ' positive ',\r\n",
    "        '♥': ' positive ', '🤩': ' positive ', 'like': ' positive ', '💌': ' positive ',\r\n",
    "        '🤣': ' positive ', '🖤': ' positive ', '🤤': ' positive ', ':(': ' nagative ', '😢': ' nagative ',\r\n",
    "        '❤': ' positive ', '😍': ' positive ', '😘': ' positive ', '😪': ' nagative ', '😊': ' positive ',\r\n",
    "        '?': ' ? ', '😁': ' positive ', '💖': ' positive ', '😟': ' nagative ', '😭': ' nagative ',\r\n",
    "        '💯': ' positive ', '💗': ' positive ', '♡': ' positive ', '💜': ' positive ', '🤗': ' positive ',\r\n",
    "        '^^': ' positive ', '😨': ' nagative ', '☺': ' positive ', '💋': ' positive ', '👌': ' positive ',\r\n",
    "        '😖': ' nagative ', '😀': ' positive ', ':((': ' nagative ', '😡': ' nagative ', '😠': ' nagative ',\r\n",
    "        '😒': ' nagative ', '🙂': ' positive ', '😏': ' nagative ', '😝': ' positive ', '😄': ' positive ',\r\n",
    "        '😙': ' positive ', '😤': ' nagative ', '😎': ' positive ', '😆': ' positive ', '💚': ' positive ',\r\n",
    "        '✌': ' positive ', '💕': ' positive ', '😞': ' nagative ', '😓': ' nagative ', '️🆗️': ' positive ',\r\n",
    "        '😉': ' positive ', '😂': ' positive ', ':v': '  positive ', '=))': '  positive ', '😋': ' positive ',\r\n",
    "        '💓': ' positive ', '😐': ' nagative ', ':3': ' positive ', '😫': ' nagative ', '😥': ' nagative ',\r\n",
    "        '😃': ' positive ', '😬': ' 😬 ', '😌': ' 😌 ', '💛': ' positive ', '🤝': ' positive ', '🎈': ' positive ',\r\n",
    "        '😗': ' positive ', '🤔': ' nagative ', '😑': ' nagative ', '🔥': ' nagative ', '🙏': ' nagative ',\r\n",
    "        '🆗': ' positive ', '😻': ' positive ', '💙': ' positive ', '💟': ' positive ',\r\n",
    "        '😚': ' positive ', '❌': ' nagative ', '👏': ' positive ', ';)': ' positive ', '<3': ' positive ',\r\n",
    "        '🌝': ' positive ',  '🌷': ' positive ', '🌸': ' positive ', '🌺': ' positive ',\r\n",
    "        '🌼': ' positive ', '🍓': ' positive ', '🐅': ' positive ', '🐾': ' positive ', '👉': ' positive ',\r\n",
    "        '💐': ' positive ', '💞': ' positive ', '💥': ' positive ', '💪': ' positive ',\r\n",
    "        '💰': ' positive ',  '😇': ' positive ', '😛': ' positive ', '😜': ' positive ',\r\n",
    "        '🙃': ' positive ', '🤑': ' positive ', '🤪': ' positive ','☹': ' nagative ',  '💀': ' nagative ',\r\n",
    "        '😔': ' nagative ', '😧': ' nagative ', '😩': ' nagative ', '😰': ' nagative ', '😳': ' nagative ',\r\n",
    "        '😵': ' nagative ', '😶': ' nagative ', '🙁': ' nagative ',\r\n",
    "        #Chuẩn hóa 1 số sentiment words/English words\r\n",
    "        ':))': '  positive ', ':)': ' positive ', 'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\r\n",
    "        'okey': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\r\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\r\n",
    "        '⭐': 'star ', '*': 'star ', '🌟': 'star ', '🎉': u' positive ',\r\n",
    "        'kg ': u' không ','not': u' không ', u' kg ': u' không ', '\"k ': u' không ',' kh ':u' không ','kô':u' không ','hok':u' không ',' kp ': u' không phải ',u' kô ': u' không ', '\"ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',\r\n",
    "        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',\r\n",
    "        ' lol ': ' nagative ',' cc ': ' nagative ','cute': u' dễ thương ','huhu': ' nagative ', ' vs ': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',\r\n",
    "        ' sz ': u' cỡ ', 'size': u' cỡ ', u' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\r\n",
    "        'đc': u' được ','authentic': u' chuẩn chính hãng ',u' aut ': u' chuẩn chính hãng ', u' auth ': u' chuẩn chính hãng ', 'thick': u' positive ', 'store': u' cửa hàng ',\r\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\r\n",
    "        'sấu': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\r\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\r\n",
    "        'ể': 'ể', 'product': 'sản phẩm', 'quality': 'chất lượng','chat':' chất ', 'excelent': 'hoàn hảo', 'bad': 'tệ','fresh': ' tươi ','sad': ' tệ ',\r\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\r\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\r\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\r\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\r\n",
    "        'dep': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ',\r\n",
    "        'iu': u' yêu ','fake': u' giả mạo ', 'trl': 'trả lời', '><': u' positive ',\r\n",
    "        ' por ': u' tệ ',' poor ': u' tệ ', 'ib':u' nhắn tin ', 'rep':u' trả lời ',u'fback':' feedback ','fedback':' feedback ',\r\n",
    "        #dưới 3* quy về 1*, trên 3* quy về 5*\r\n",
    "        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\r\n",
    "        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\r\n",
    "        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}\r\n",
    "\r\n",
    "    for k, v in replace_list.items():\r\n",
    "        text = text.replace(k, v)\r\n",
    "\r\n",
    "    # chuyen punctuation thành space\r\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\r\n",
    "    text = text.translate(translator)\r\n",
    "\r\n",
    "    text = ViTokenizer.tokenize(text)\r\n",
    "    texts = text.split()\r\n",
    "    len_text = len(texts)\r\n",
    "\r\n",
    "    texts = [t.replace('_', ' ') for t in texts]\r\n",
    "    for i in range(len_text):\r\n",
    "        cp_text = texts[i]\r\n",
    "        if cp_text in not_list: # Xử lý vấn đề phủ định (VD: áo này chẳng đẹp--> áo này notpos)\r\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\r\n",
    "\r\n",
    "            for j in range(numb_word):\r\n",
    "                if texts[i + j + 1] in pos_list:\r\n",
    "                    texts[i] = 'notpos'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "\r\n",
    "                if texts[i + j + 1] in nag_list:\r\n",
    "                    texts[i] = 'notnag'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "        else: #Thêm feature cho những sentiment words (áo này đẹp--> áo này đẹp positive)\r\n",
    "            if cp_text in pos_list:\r\n",
    "                texts.append('positive')\r\n",
    "            elif cp_text in nag_list:\r\n",
    "                texts.append('nagative')\r\n",
    "\r\n",
    "    text = u' '.join(texts)\r\n",
    "\r\n",
    "    #remove nốt những ký tự thừa thãi\r\n",
    "    text = text.replace(u'\"', u' ')\r\n",
    "    text = text.replace(u'️', u'')\r\n",
    "    text = text.replace('🏻','')\r\n",
    "    return text\r\n",
    "\r\n",
    "#print(normalize_text('Nma kính đẹp chắc chắn 😎'))\r\n",
    "\r\n",
    "class DataSource(object):\r\n",
    "\r\n",
    "    def _load_raw_data(self, filename, is_train=True):\r\n",
    "\r\n",
    "        a = []\r\n",
    "        b = []\r\n",
    "\r\n",
    "        regex = 'train_'\r\n",
    "        if not is_train:\r\n",
    "            regex = 'test_'\r\n",
    "\r\n",
    "        with open(filename, 'r', encoding='UTF-8') as file:\r\n",
    "            for line in file:\r\n",
    "                if regex in line:\r\n",
    "                    b.append(a)\r\n",
    "                    a = [line]\r\n",
    "                elif line != '\\n':\r\n",
    "                    a.append(line)\r\n",
    "        b.append(a)\r\n",
    "\r\n",
    "        return b[1:]\r\n",
    "\r\n",
    "    def _create_row(self, sample, is_train=True):\r\n",
    "\r\n",
    "        d = {}\r\n",
    "        d['id'] = sample[0].replace('\\n', '')\r\n",
    "        review = \"\"\r\n",
    "\r\n",
    "        if is_train:\r\n",
    "            for clause in sample[1:-1]:\r\n",
    "                review += clause.replace('\\n', ' ')\r\n",
    "                review = review.replace('.', ' ')\r\n",
    "\r\n",
    "            d['label'] = int(sample[-1].replace('\\n', ' '))\r\n",
    "        else:\r\n",
    "            for clause in sample[1:]:\r\n",
    "                review += clause.replace('\\n', ' ')\r\n",
    "                review = review.replace('.', ' ')\r\n",
    "\r\n",
    "\r\n",
    "        d['review'] = review\r\n",
    "\r\n",
    "        return d\r\n",
    "\r\n",
    "    def load_data(self, filename, is_train=True):\r\n",
    "\r\n",
    "        raw_data = self._load_raw_data(filename, is_train)\r\n",
    "        lst = []\r\n",
    "\r\n",
    "        for row in raw_data:\r\n",
    "            lst.append(self._create_row(row, is_train))\r\n",
    "\r\n",
    "        return lst\r\n",
    "\r\n",
    "    def transform_to_dataset(self, x_set,y_set):\r\n",
    "        X, y = [], []\r\n",
    "        for document, topic in zip(list(x_set), list(y_set)):\r\n",
    "            document = normalize_text(document)\r\n",
    "            X.append(document.strip())\r\n",
    "            y.append(topic)\r\n",
    "            #Augmentation bằng cách remove dấu tiếng Việt\r\n",
    "            X.append(no_marks(document))\r\n",
    "            y.append(topic)\r\n",
    "        return X, y\r\n",
    "\r\n",
    "ds = DataSource()\r\n",
    "train_data = pd.DataFrame(ds.load_data('./sentiment/data_clean/train.crash'))\r\n",
    "new_data = []\r\n",
    "\r\n",
    "#Thêm mẫu bằng cách lấy trong từ điển Sentiment (nag/pos)\r\n",
    "for index,row in enumerate(nag_list):\r\n",
    "    new_data.append(['pos'+str(index),'0',row])\r\n",
    "for index,row in enumerate(nag_list):\r\n",
    "    new_data.append(['nag'+str(index),'1',row])\r\n",
    "\r\n",
    "new_data = pd.DataFrame(new_data,columns=list(['id','label','review']))\r\n",
    "train_data.append(new_data)\r\n",
    "test_data = pd.DataFrame(ds.load_data('./sentiment/data_clean/test.crash', is_train=False))\r\n",
    "\r\n",
    "#Try some models\r\n",
    "classifiers = [\r\n",
    "            # MultinomialNB(),\r\n",
    "            # DecisionTreeClassifier(),\r\n",
    "            # LogisticRegression(),\r\n",
    "            # SGDClassifier(),\r\n",
    "            LinearSVC(fit_intercept = True,multi_class='crammer_singer', C=1),\r\n",
    "        ]\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.review, train_data.label, test_size=0.3,random_state=42)\r\n",
    "X_train, y_train = ds.transform_to_dataset(X_train,y_train)\r\n",
    "X_test, y_test = ds.transform_to_dataset(X_test, y_test)\r\n",
    "\r\n",
    "#THÊM STOPWORD LÀ NHỮNG TỪ KÉM QUAN TRỌNG\r\n",
    "stop_ws = (u'rằng',u'thì',u'là',u'mà')\r\n",
    "\r\n",
    "for classifier in classifiers:\r\n",
    "    steps = []\r\n",
    "    steps.append(('CountVectorizer', CountVectorizer(ngram_range=(1,5),stop_words=stop_ws,max_df=0.5, min_df=5)))\r\n",
    "    steps.append(('tfidf', TfidfTransformer(use_idf=False, sublinear_tf = True,norm='l2',smooth_idf=True)))\r\n",
    "    steps.append(('classifier', classifier))\r\n",
    "    clf = Pipeline(steps)\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "    y_pred = clf.predict(X_test)\r\n",
    "    report1 = metrics.classification_report(y_test, y_pred, labels=[1,0], digits=3)\r\n",
    "\r\n",
    "X_train, y_train = ds.transform_to_dataset(train_data.review, train_data.label)\r\n",
    "\r\n",
    "\r\n",
    "#TRAIN OVERFITTING/ERRO ANALYSIS\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "y_pred = clf.predict(X_train)\r\n",
    "report2 = metrics.classification_report(y_train, y_pred, labels=[1,0], digits=3)\r\n",
    "\r\n",
    "#ERRO ANALYSIS\r\n",
    "#for id,x, y1, y2 in zip(train_data.id, X_train, y_train, y_pred):\r\n",
    "    #if y1 != y2:\r\n",
    "        # CHECK EACH WRONG SAMPLE POSSITIVE/NAGATIVE\r\n",
    "        #if y1!=1:#0:\r\n",
    "            #print(id,x, y1, y2)\r\n",
    "\r\n",
    "#CROSS VALIDATION\r\n",
    "cross_score = cross_val_score(clf, X_train,y_train, cv=5)\r\n",
    "\r\n",
    "#REPORT\r\n",
    "print('DATASET LEN %d'%(len(X_train)))\r\n",
    "print('TRAIN 70/30 \\n\\n',report1)\r\n",
    "print('TRAIN OVERFITING\\n\\n',report2)\r\n",
    "print(\"CROSSVALIDATION 5 FOLDS: %0.4f (+/- %0.4f)\" % (cross_score.mean(), cross_score.std() * 2))\r\n",
    "\r\n",
    "# #SAVE FILE SUBMIT\r\n",
    "#test_list = []\r\n",
    "#for document in test_data.review:\r\n",
    "#    document = normalize_text(document)\r\n",
    "#    test_list.append(document)\r\n",
    "#print(test_list)\r\n",
    "#y_predict = clf.predict(test_list)\r\n",
    "#test_data['label'] = y_predict\r\n",
    "# test_data['content'] = test_list\r\n",
    "#test_data = test_data.sort_values(by=['label'])\r\n",
    "#test_data[['id', 'label']].to_csv('submit.csv', index=False)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:928: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:928: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DATASET LEN 32146\n",
      "TRAIN 70/30 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.938     0.945     0.941      4438\n",
      "           0      0.952     0.947     0.950      5206\n",
      "\n",
      "    accuracy                          0.946      9644\n",
      "   macro avg      0.945     0.946     0.946      9644\n",
      "weighted avg      0.946     0.946     0.946      9644\n",
      "\n",
      "TRAIN OVERFITING\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.981     0.985     0.983     14766\n",
      "           0      0.987     0.984     0.985     17380\n",
      "\n",
      "    accuracy                          0.984     32146\n",
      "   macro avg      0.984     0.984     0.984     32146\n",
      "weighted avg      0.984     0.984     0.984     32146\n",
      "\n",
      "CROSSVALIDATION 5 FOLDS: 0.9436 (+/- 0.0096)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Thứ tự chạy: 2\r\n",
    "#Phần lọc itemID và shopID ra từ link\r\n",
    "def getID(URL):\r\n",
    "    group = ''\r\n",
    "    itemID = shopID = ''\r\n",
    "    chk = ''\r\n",
    "    cnt = 0\r\n",
    "    isdone = False\r\n",
    "    for i in URL:\r\n",
    "        if chk != 'i.':\r\n",
    "            if i != '-':\r\n",
    "                group += i\r\n",
    "                if group == 'i.':\r\n",
    "                    chk = group\r\n",
    "            else: group = ''\r\n",
    "        else:\r\n",
    "            if isdone == False:\r\n",
    "                if i != '.':\r\n",
    "                    shopID += i\r\n",
    "                else: isdone = True\r\n",
    "            else:\r\n",
    "                if i in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\r\n",
    "                    itemID += i\r\n",
    "                else: return itemID, shopID\r\n",
    "    return itemID, shopID\r\n",
    "print(getID('https://shopee.vn/Kính-cận-đổi-màu-mắt-kính-nam-cận-râm-2-in1-mắt-cận-đổi-màu-theo-ánh-nắng-i.18722862.5560946376?adsid=0&campaignid=0&position=1'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('5560946376', '18722862')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Thứ tự chạy: 3\r\n",
    "\r\n",
    "#Lấy các review từ itemID và shopID đã có ở trên thông qua API Shopee\r\n",
    "import requests, time\r\n",
    "from IPython.display import display, clear_output\r\n",
    "\r\n",
    "\r\n",
    "headers = {\r\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\"\r\n",
    "}\r\n",
    "\r\n",
    "if (int(input('Chọn 1 để nhập id, chọn 2 để nhập URL:')) == 1):\r\n",
    "    itemID = input('Nhập id item:')\r\n",
    "    shopID = input('Nhập id shop:')\r\n",
    "else:\r\n",
    "    productURL = input('Nhap URL san pham:')\r\n",
    "#itemID = input('Nhap id cua item:')\r\n",
    "#shopID = input('Nhap id cua shop:')\r\n",
    "\r\n",
    "    itemID, shopID = getID(productURL)\r\n",
    "\r\n",
    "\r\n",
    "path_not = './sentiment/sentiment_dicts/not.txt'\r\n",
    "with codecs.open(path_not, 'r', encoding='UTF-8') as f:\r\n",
    "    not_ = f.readlines()\r\n",
    "not_list = [n.replace('\\n', '') for n in not_]\r\n",
    "api = \"https://shopee.vn/api/v2/item/get_ratings?filter=0&flag=1&type=0&limit=6\"\r\n",
    "api += '&itemid=' + itemID + '&shopid=' + shopID\r\n",
    "r = requests.get(api).json()\r\n",
    "api = \"https://shopee.vn/api/v2/item/get?\" + 'itemid=' + itemID + '&shopid=' + shopID\r\n",
    "#print(api)\r\n",
    "r1 = requests.get(api, headers=headers).json()\r\n",
    "#print(r1)\r\n",
    "tensp = r1['item']['name']\r\n",
    "rating_star = 0\r\n",
    "\r\n",
    "\r\n",
    "rv_num = r['data']['item_rating_summary']['rating_total']\r\n",
    "rcount_text = r['data']['item_rating_summary']['rcount_with_context']\r\n",
    "data = []\r\n",
    "#data.append({'ID san pham:': itemID, 'ID shop:': shopID, 'Tong danh gia:': rv_num})\r\n",
    "offset_length = 0\r\n",
    "i = 1\r\n",
    "chk_empty = 0\r\n",
    "start = time.time()\r\n",
    "estimate_start = time.time()\r\n",
    "estimate_time = 0\r\n",
    "time_step = 0.5\r\n",
    "rv_count = 1\r\n",
    "rv_left = rcount_text\r\n",
    "while i < rcount_text:\r\n",
    "    if (time.time() - start > 6000):\r\n",
    "        break\r\n",
    "    if (time.time() - estimate_start > time_step):\r\n",
    "        estimate_time = rv_left/(rv_count/time_step)\r\n",
    "        rv_left -= rv_count\r\n",
    "        estimate_start = time.time()\r\n",
    "        rv_count = 1\r\n",
    "    api = \"https://shopee.vn/api/v2/item/get_ratings?filter=0&flag=1&type=0&limit=6\"\r\n",
    "    api += '&itemid=' + itemID + '&shopid=' + shopID + '&offset=' + str(offset_length)\r\n",
    "    r = requests.get(api).json()\r\n",
    "    if r['data']['ratings'] == []:\r\n",
    "        print(r)\r\n",
    "        break\r\n",
    "    #print(r['data']['ratings'])\r\n",
    "    clear_output(wait=True)\r\n",
    "    print('Ước tính thời gian còn lại:', estimate_time//60, 'phút', estimate_time%60,'giây')\r\n",
    "    print('Load review: ', i, ' offset:', offset_length)\r\n",
    "    for departure in r['data']['ratings']:\r\n",
    "        if departure['comment'] == '':\r\n",
    "            chk_empty += 1\r\n",
    "            continue\r\n",
    "        dem_tu = 0\r\n",
    "        for chu in departure['comment']:\r\n",
    "            if chu == ' ':\r\n",
    "                dem_tu += 1\r\n",
    "            if dem_tu >= 13:\r\n",
    "                break\r\n",
    "        if dem_tu < 13 or dem_tu >500: \r\n",
    "            offset_length += 1\r\n",
    "            continue\r\n",
    "        data.append({\r\n",
    "        'Number': i,\r\n",
    "        'Nguoi dung': departure['author_username'],\r\n",
    "        'Nhan xet': departure['comment'],\r\n",
    "        'Sao': departure['rating_star']\r\n",
    "        })\r\n",
    "        #print(estimate_time)\r\n",
    "        #print(\"\\nNumber: \", data[i-1]['Number'])\r\n",
    "        #print(\"Nguoi dung: \", data[i-1]['Nguoi dung'])\r\n",
    "        #print(\"Nhan xet: \", data[i-1]['Nhan xet'])\r\n",
    "        #print(\"Sao: \", data[i-1]['Sao'])\r\n",
    "        #print(\"-----------------\")\r\n",
    "\r\n",
    "        i += 1\r\n",
    "        rv_count += 1\r\n",
    "\r\n",
    "        offset_length += 1\r\n",
    "    if chk_empty > 5:\r\n",
    "        #i += 1\r\n",
    "        offset_length +=1\r\n",
    "        chk_empty = 0\r\n",
    "    #rv_num -= 6\r\n",
    "clear_output()\r\n",
    "up = 1\r\n",
    "for x in r['data']['item_rating_summary']['rating_count']:\r\n",
    "    rating_star += up * x\r\n",
    "    up += 1\r\n",
    "rating_star /= r['data']['item_rating_summary']['rating_total']\r\n",
    "print('Thời gian chạy:', (time.time() - start)//60, 'phút', (time.time() - start)%60,'giây')\r\n",
    "print('Sao trung bình:', rating_star)\r\n",
    "print('Tên sản phẩm:', tensp)\r\n",
    "print('Load review:', i)\r\n",
    "print(\"ID san pham:\", itemID, \"\\nID shop:\", shopID, \"\\nTong danh gia:\", rv_num, \"\\nTong danh gia co binh luan:\", rcount_text)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thời gian chạy: 0.0 phút 51.61233448982239 giây\n",
      "Sao trung bình: 4.879432624113475\n",
      "Tên sản phẩm: Apple iPad Air 4 10.9 inch Wi-Fi\n",
      "Load review: 150\n",
      "ID san pham: 5076952578 \n",
      "ID shop: 88201679 \n",
      "Tong danh gia: 423 \n",
      "Tong danh gia co binh luan: 231\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import unidecode\r\n",
    "\r\n",
    "def SentenceSpliter(sentence):\r\n",
    "    split = []\r\n",
    "    word = ''\r\n",
    "    for i in range(0, len(sentence)):\r\n",
    "        if sentence[i] == ' ':\r\n",
    "            split.append(word)\r\n",
    "            word = ''\r\n",
    "        elif i == len(sentence) - 1 and word != '':\r\n",
    "            word += sentence[i]\r\n",
    "            split.append(word)\r\n",
    "        else: word += sentence[i]\r\n",
    "    return split\r\n",
    "\r\n",
    "def SentenceCombiner(split):\r\n",
    "    sentence = ''\r\n",
    "    for i in range(0, len(split)):\r\n",
    "        if i != len(split)-1:\r\n",
    "            sentence += split[i] + ' '\r\n",
    "        else:\r\n",
    "            sentence += split[i]\r\n",
    "    return sentence\r\n",
    "\r\n",
    "def XoaDau(word, syllable):\r\n",
    "    rmv_accent = word\r\n",
    "    for x in rmv_accent:\r\n",
    "        for s in syllable:\r\n",
    "            if x != s:\r\n",
    "                #print(syllable[s])\r\n",
    "                for j in syllable[s]:\r\n",
    "                    if x == j:\r\n",
    "                        rmv_accent = rmv_accent.replace(x, s)\r\n",
    "    return rmv_accent\r\n",
    "\r\n",
    "def VanNguyenAm(word, syllable):\r\n",
    "    #Vần đơn giản 1: chỉ chứa 1 nguyên âm\r\n",
    "    correct = False\r\n",
    "    if len(word) == 1:\r\n",
    "            while (correct != True):\r\n",
    "                for x in syllable:\r\n",
    "                    if x in ['e', 'a', 'o', 'ê', 'ơ', 'ô', 'i', 'y', 'ư', 'u']:\r\n",
    "                        if word != x:\r\n",
    "                            for y in range(0, 5):\r\n",
    "                                if word == syllable[x][y]:\r\n",
    "                                    correct = True\r\n",
    "                        else:\r\n",
    "                            correct = True\r\n",
    "                    else: continue\r\n",
    "                break\r\n",
    "    return correct\r\n",
    "\r\n",
    "#Vần đơn giản (2): Nguyên âm + phụ âm cuối ~ Vần ngược\r\n",
    "def VanDonGian2(word, syllable, vowel, consonant):\r\n",
    "    bangVanLoai2 = [[1, 1, 1, 1, 0, 1, 1, 1],\r\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0],\r\n",
    "                    [1, 0, 0, 1, 1, 0, 0, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [0, 1, 1, 1, 0, 1, 1, 1],\r\n",
    "                    [0, 0, 1, 1, 0, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1]]\r\n",
    "    correct = [False, False]\r\n",
    "    for j in syllable:\r\n",
    "        if word[0] == j:\r\n",
    "            x = vowel['singleVowel'].index(word[0])\r\n",
    "            correct[0] = True\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            for t in range(0, 5):\r\n",
    "                if word[0] == syllable[j][t]:\r\n",
    "                    x = vowel['singleVowel'].index(j)\r\n",
    "                    correct[0] = True\r\n",
    "                    break\r\n",
    "    if correct[0] == True:\r\n",
    "        if word[1:] in consonant['tConsonant']:\r\n",
    "            y = consonant['tConsonant'].index(word[1:])\r\n",
    "            correct[1] = True\r\n",
    "        if correct[1] == True:\r\n",
    "            if bangVanLoai2[x][y] != 1:\r\n",
    "                return False\r\n",
    "            return True\r\n",
    "    return False\r\n",
    "\r\n",
    "#Kiểm tra vần hòa âm và hợp âm\r\n",
    "def KtraHoaAmHopAm(word, syllable, consonant):\r\n",
    "    rmv_accent = word\r\n",
    "    tailConsonant = ''\r\n",
    "    chk = ''\r\n",
    "    for i in range(1,3):\r\n",
    "        chk += rmv_accent[i-3]\r\n",
    "        if chk in consonant['tConsonant']:\r\n",
    "            tailConsonant = chk\r\n",
    "            #rmv_accent.replace(rmv_accent[-1], '')\r\n",
    "        else: chk = ''\r\n",
    "    #print(tailConsonant)\r\n",
    "    for i in range(len(tailConsonant)):\r\n",
    "        rmv_accent = rmv_accent.rstrip(rmv_accent[-1])\r\n",
    "    #print(rmv_accent)\r\n",
    "    if rmv_accent in ['eo', 'ao', 'ai', 'oe', 'oa', 'oi', 'êu', 'ơi', 'ôi', 'ia',\r\n",
    "    'iu', 'ưa', 'ưi', 'ưu', 'ua', 'uê', 'ui', 'uy']:\r\n",
    "        return True\r\n",
    "\r\n",
    "    if rmv_accent in ['ay', 'ây', 'au', 'âu']:\r\n",
    "        if tailConsonant == '':\r\n",
    "            return True\r\n",
    "        return False\r\n",
    "    if rmv_accent in ['oă', 'uâ', 'iê', 'yê', 'ươ', 'uô']:\r\n",
    "        if tailConsonant != '':\r\n",
    "            return True\r\n",
    "    if rmv_accent in ['uôi', 'ươi', 'ươu','yêu', 'iêu']:\r\n",
    "        return True\r\n",
    "    return False\r\n",
    "\r\n",
    "def SentenceCorrector(splits):\r\n",
    "    #split có dạng: split = ['từ', 'ví, 'dụ'...]\r\n",
    "    #singleVowel: Nguyên âm đơn -> Có thể đứng 1 mình hoặc ghép với những chữ khác\r\n",
    "    #sVowel: Bán nguyên âm -> Không thể đứng 1 mình hay đứng cuối mà phải có phụ âm cuối đi theo\r\n",
    "    #hConsonant: Phụ âm đầu: đứng trước nguyên âm\r\n",
    "    #tConsonant: Phụ âm cuối: đứng sau nguyên âm\r\n",
    "    \r\n",
    "    vowel = {'singleVowel':['i', 'y', 'ư', 'u', 'ê', 'ơ', 'â', 'ô', 'e', 'a', 'ă', 'o'],\r\n",
    "    'sVowel':['ă', 'â', 'iê', 'yê', 'uô', 'ươ']}\r\n",
    "    consonant = {'hConsonant':['b', 'c', 'd', 'đ', 'g', 'gh', 'h', 'k', 'l', 'm',\r\n",
    "    'n', 'r', 's', 't', 'v', 'x', 'ch' , 'gi', 'kh', 'nh', 'ng', 'ngh', 'qu', 'ph',\r\n",
    "    'th', 'tr'], \r\n",
    "    'tConsonant': ['c', 'ch', 'm', 'n', 'ng', 'nh', 'p', 't']}\r\n",
    "    syllable = {'i':['í', 'ì', 'ỉ', 'ĩ', 'ị'], 'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ'],\r\n",
    "    'ư': ['ứ', 'ừ', 'ử', 'ữ', 'ự'], 'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ'],\r\n",
    "    'ê': ['ế', 'ề', 'ể', 'ễ', 'ệ'], 'ơ': ['ớ', 'ờ', 'ở', 'ỡ', 'ợ'],\r\n",
    "    'â': ['ấ', 'ầ', 'ẩ', 'ẫ', 'ậ'], 'ô': ['ố', 'ồ', 'ổ', 'ỗ', 'ộ'],\r\n",
    "    'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ'], 'a': ['á', 'à', 'ả', 'ã', 'ạ'],\r\n",
    "    'ă': ['ắ', 'ằ', 'ẳ', 'ẵ', 'ặ'], 'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ']}\r\n",
    "    rmv = -1\r\n",
    "\r\n",
    "    split = splits.copy()\r\n",
    "    for i in splits:\r\n",
    "        rmv +=1\r\n",
    "    #Ktra điều kiện: Từ = vần Nguyên Âm:\r\n",
    "        if (len(i)==1):\r\n",
    "            #print('1 tu:', i)\r\n",
    "            if VanNguyenAm(i, syllable) != True:\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "        else:\r\n",
    "            #Tách phụ âm đầu\r\n",
    "            #pad = True\r\n",
    "            headConsonant = ''\r\n",
    "            chk = ''\r\n",
    "            cnt = 0\r\n",
    "            for y in i:\r\n",
    "                cnt += 1\r\n",
    "                chk += y\r\n",
    "                if chk in consonant['hConsonant']:\r\n",
    "                    headConsonant = chk\r\n",
    "                else:\r\n",
    "                    if cnt > 1: break\r\n",
    "                #if headConsonant == '': \r\n",
    "                #    split.pop(rmv)\r\n",
    "                #    pad = False\r\n",
    "                #    break\r\n",
    "            #if pad == False:\r\n",
    "            #    continue\r\n",
    "            #print('tách phụ âm đầu: ', headConsonant)\r\n",
    "            i = i.replace(headConsonant, '')\r\n",
    "            i = XoaDau(i, syllable)\r\n",
    "            #print(i)\r\n",
    "            if (len(i)<1):\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -=1\r\n",
    "                continue\r\n",
    "            if (len(i) == 1):\r\n",
    "                #print('TH1:', i)\r\n",
    "                if VanNguyenAm(i, syllable) == True:\r\n",
    "                    continue\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "                continue\r\n",
    "            if VanDonGian2(i, syllable, vowel, consonant) == True:\r\n",
    "                #print('TH2:', i)\r\n",
    "                continue\r\n",
    "            elif KtraHoaAmHopAm(i, syllable, consonant) == True:\r\n",
    "                #print('TH3:', i)\r\n",
    "                continue\r\n",
    "            else: \r\n",
    "                #print('TH5:', i)\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "    return split\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "sentence = \"phân loại hàng silver ipad air 4 có kiểu dáng mới tương tự iPad Pro 2020 nhưng có kích thước nhỏ hơn và dày chỉ 6.1 mm, trọng lượng đạt 460 g dễ dàng mang theo bên mình mọi lúc mọi nơi thiết kế này giúp tương thích với bàn phím Apple Smart Keyboard Folio Magic Keyboard của iPad Pro 11 inch và hỗ trợ bút Apple Pencil 2\"\r\n",
    "print(SentenceSpliter(sentence))\r\n",
    "print(SentenceCombiner(SentenceCorrector(SentenceSpliter(sentence))))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['phân', 'loại', 'hàng', 'silver', 'ipad', 'air', '4', 'có', 'kiểu', 'dáng', 'mới', 'tương', 'tự', 'iPad', 'Pro', '2020', 'nhưng', 'có', 'kích', 'thước', 'nhỏ', 'hơn', 'và', 'dày', 'chỉ', '6.1', 'mm,', 'trọng', 'lượng', 'đạt', '460', 'g', 'dễ', 'dàng', 'mang', 'theo', 'bên', 'mình', 'mọi', 'lúc', 'mọi', 'nơi', 'thiết', 'kế', 'này', 'giúp', 'tương', 'thích', 'với', 'bàn', 'phím', 'Apple', 'Smart', 'Keyboard', 'Folio', 'Magic', 'Keyboard', 'của', 'iPad', 'Pro', '11', 'inch', 'và', 'hỗ', 'trợ', 'bút', 'Apple', 'Pencil']\n",
      "phân hàng có kiểu dáng mới tương tự nhưng có kích thước nhỏ hơn và dày chỉ trọng lượng đạt dễ dàng mang theo bên mình mọi lúc mọi nơi thiết kế này giúp tương thích với bàn phím của và hỗ trợ bút\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Thứ tự chạy: 4\r\n",
    "\r\n",
    "import re\r\n",
    "import string\r\n",
    "\r\n",
    "#Làm sạch dữ liệu review\r\n",
    "def normalize_text(texts):\r\n",
    "    \r\n",
    "    #Chuẩn hóa tiếng Việt, xử lý emoj, chuẩn hóa tiếng Anh, thuật ngữ\r\n",
    "    replace_list = {\r\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\r\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\r\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\r\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\r\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\r\n",
    "        #Chuẩn hóa 1 số sentiment words/English words\r\n",
    "        ':)) ': ' hài lòng ', ' shiper ': ' người_giao_hàng ', ' shipper ': ' người_giao_hàng ', ' giao hàng ': ' giao_hàng ' , ' mng ': ' mọi người ', ' tnao ': ' thế nào ' , ' m.n ': ' mọi người' , ' tr ': ' trời ', ' nma ': ' nhưng mà ', ' mn ': ' mọi người ', ' mk ': ' mình ' , ':)': ' positive ', 'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\r\n",
    "        'okey': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\r\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\r\n",
    "        'kg ': u' không ','not': u' không ', u' kg ': u' không ', '\"k ': u' không ',' kh ':u' không ','kô':u' không ','hok':u' không ',' kp ': u' không phải ',u' kô ': u' không ', '\"ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',\r\n",
    "        'he he': ' thích ','hehe': ' thích ','hihi': ' thích ', 'haha': ' thích ', 'hjhj': ' thích ',\r\n",
    "        ' lol ': ' tệ ',' cc ': ' tệ ','cute': u' dễ thương ','huhu': ' tệ ', ' vs ': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',\r\n",
    "        ' sz ': u' cỡ ', 'size': u' cỡ ', u' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\r\n",
    "        'đc': u' được ','authentic': u' chuẩn chính hãng ',u' aut ': u' chuẩn chính hãng ', u' auth ': u' chuẩn chính hãng ', 'thick': u' thích ', 'store': u' cửa hàng ',\r\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\r\n",
    "        'sấu': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\r\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\r\n",
    "        'ể': 'ể', 'product': 'sản phẩm', 'quality': 'chất lượng','chat':' chất ', 'excelent': 'hoàn hảo', 'bad': 'tệ','fresh': ' tươi ','sad': ' tệ ',\r\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\r\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\r\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\r\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\r\n",
    "        'dep': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ',\r\n",
    "        'iu': u' yêu ','fake': u' giả mạo ', 'trl': 'trả lời', '><': u' thích ',\r\n",
    "        ' por ': u' tệ ',' poor ': u' tệ ', 'ib':u' nhắn tin ', 'rep':u' trả lời ',u'fback':' feedback ','fedback':' feedback ',\r\n",
    "        #dưới 3* quy về 1*, trên 3* quy về 5*\r\n",
    "        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\r\n",
    "        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\r\n",
    "        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}\r\n",
    "    \r\n",
    "    sents = []\r\n",
    "    cau = '';\r\n",
    "    count = 0\r\n",
    "    for text in texts:\r\n",
    "        count += 1\r\n",
    "        #print(text['Nhan xet'])\r\n",
    "        cau = text['Nhan xet']\r\n",
    "        #cau = normalize_text(cau)\r\n",
    "        sao = text['Sao']\r\n",
    "        #Sửa các icon, từ viết tắt\r\n",
    "        if cau == None:\r\n",
    "            cau = ''\r\n",
    "        for k, v in replace_list.items():\r\n",
    "                cau = cau.replace(k, v)\r\n",
    "        #print(cau)\r\n",
    "        #Remove các ký tự kéo dài: vd: đẹppppppp\r\n",
    "        texts = cau.split()\r\n",
    "        len_text = len(texts)\r\n",
    "        for i in range(len_text):\r\n",
    "            cp_text = texts[i]\r\n",
    "        if cp_text in not_list: # Xử lý vấn đề phủ định (VD: áo này chẳng đẹp--> áo này notpos)\r\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\r\n",
    "\r\n",
    "            for j in range(numb_word):\r\n",
    "                if texts[i + j + 1] in pos_list:\r\n",
    "                    texts[i] = 'notpos'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "\r\n",
    "                if texts[i + j + 1] in nag_list:\r\n",
    "                    texts[i] = 'notnag'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "        #else: #Thêm feature cho những sentiment words (áo này đẹp--> áo này đẹp positive)\r\n",
    "            #if cp_text in pos_list:\r\n",
    "            #    texts.append('')\r\n",
    "            #if cp_text in nag_list:\r\n",
    "            #    texts.append('nagative')\r\n",
    "        \r\n",
    "        cau = u' '.join(texts)\r\n",
    "        cau = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), cau, flags=re.IGNORECASE)\r\n",
    "        preinput = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', cau)\r\n",
    "        pre = dict()\r\n",
    "        \r\n",
    "        #Xóa kí tự đặc biệt\r\n",
    "        for i in preinput:\r\n",
    "            pre['Nhan xet'] = re.sub(r'[^a-zA-ZÀ-ỹ\\s]', '', i, re.I|re.A).lower()\r\n",
    "            pre['Nhan xet'] = SentenceCombiner(SentenceCorrector(SentenceSpliter(i)))\r\n",
    "            pre['Sao'] = sao\r\n",
    "        sents.append(pre)\r\n",
    "        clear_output(wait=True)\r\n",
    "        print('Review', count , ':', pre['Nhan xet'])\r\n",
    "        print('.\\n.\\n.')\r\n",
    "    return sents\r\n",
    "\r\n",
    "def pre_process(raw_data):\r\n",
    "    data = normalize_text(raw_data)\r\n",
    "    #data.pop()\r\n",
    "    return data\r\n",
    "\r\n",
    "#Bước chuẩn hóa dữ liệu\r\n",
    "#for r in range(len(seg)):\r\n",
    "#    seg[r] = normalize_text(seg[r])\r\n",
    "seg = pre_process(data)\r\n",
    "#print(seg[:5]) #Dòng này để xem các review đã qua tiền xử lý\r\n",
    "print('Đã xử lý xong các review.')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Review 149 : quá cửa hàng đóng gói kỹ có điều giao hơi nhiệt dễ\n",
      ".\n",
      ".\n",
      ".\n",
      "Đã xử lý xong các review.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Thứ tự chạy: 5\r\n",
    "#Tokenize các từ trong review\r\n",
    "from pyvi import ViTokenizer\r\n",
    "segs = []\r\n",
    "cnt = 1\r\n",
    "for token in seg:\r\n",
    "    sub_segs = []\r\n",
    "    clear_output(wait=True)\r\n",
    "    #print('Xử lý review:', cnt, ':', token['Nhan xet'])\r\n",
    "    cnt += 1\r\n",
    "    doc = ViTokenizer.tokenize(str(token['Nhan xet']))\r\n",
    "    #print(pos_tag(str(sub))\r\n",
    "    sub_segs.append(str(doc))\r\n",
    "    segs.append(sub_segs)\r\n",
    "#print(segs[:5])\r\n",
    "print('Hoàn thành tokenize.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hoàn thành tokenize.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#Thứ tự chạy: 6\r\n",
    "#Chứa các đánh giá đã được phân lớp\r\n",
    "review_class = []\r\n",
    "\r\n",
    "\r\n",
    "#Tách câu:\r\n",
    "def sentence_segment_original(text):\r\n",
    "    sents = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', text)\r\n",
    "    return sents\r\n",
    "\r\n",
    "#Xử lý\r\n",
    "for rv in segs:\r\n",
    "    singleRVClass = []\r\n",
    "    y_pred = clf.predict(rv)\r\n",
    "    for i in y_pred:\r\n",
    "        singleRVClass.append(i)\r\n",
    "    review_class.append(singleRVClass)\r\n",
    "#print(review_class)\r\n",
    "\r\n",
    "#Tinh tong % tot / xau tren tat ca review\r\n",
    "def Calculate_Review_Score(pred):\r\n",
    "    score = []\r\n",
    "    for i in pred:\r\n",
    "        sum = 0\r\n",
    "        for j in i:\r\n",
    "            if j == 0:\r\n",
    "                sum += 1\r\n",
    "        score.append(sum/len(i))\r\n",
    "    #print(score)\r\n",
    "    for i in score:\r\n",
    "        sum += i\r\n",
    "    sum /= len(score)\r\n",
    "    return(sum)\r\n",
    "    #if sum < 0.3:\r\n",
    "    #    return 'Sản phẩm được đánh giá rất tệ'\r\n",
    "    #elif sum < 0.4:\r\n",
    "    #    return 'Sản phẩm được đánh giá tệ'\r\n",
    "    #elif sum <= 0.5:\r\n",
    "    #    return 'Sản phẩm được đánh giá khá tệ'\r\n",
    "    #elif sum < 0.7:\r\n",
    "    #    return 'Sản phẩm được đánh giá khá tốt'\r\n",
    "    #elif sum < 0.85:\r\n",
    "    #    return 'Sản phẩm được đánh giá tốt'\r\n",
    "    #else:\r\n",
    "    #    return 'Sản phẩm được đánh giá rất tốt'\r\n",
    "\r\n",
    "Product_Score = Calculate_Review_Score(review_class)\r\n",
    "if Product_Score <= 0.5:\r\n",
    "    good_bad = 1\r\n",
    "else: good_bad = 0\r\n",
    "print('Điểm đánh giá sản phẩm dựa trên review (thang điểm 10): ', Product_Score*10)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Điểm đánh giá sản phẩm dựa trên review (thang điểm 10):  6.308724832214764\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Thứ tự chạy: 7\r\n",
    "\r\n",
    "### SECTION LOAI BO STOPWORDS ###\r\n",
    "from underthesea import pos_tag\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "#Thêm dữ liệu từ file\r\n",
    "file = open('./data/vietnamese_stopwords.txt','r', encoding = \"utf8\")\r\n",
    "stopwords = file.read()\r\n",
    "df = pd.read_csv('./data/600_adj_pairs.txt', delimiter=\"\\t\", sep=\" \", encoding = \"utf8\", index_col=\"Relation\")\r\n",
    "df.columns = ['Word1', 'Word2']\r\n",
    "SYN = [] #Danh sách từ đồng nghĩa\r\n",
    "#print(df)\r\n",
    "SYN = df.loc[\"SYN\"]\r\n",
    "#print(SYN['Word1'])\r\n",
    "\r\n",
    "\r\n",
    "segg = segs.copy()\r\n",
    "#print(stopwords)\r\n",
    "stopword = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', stopwords)\r\n",
    "i = x =0\r\n",
    "\r\n",
    "#Thay thế khoảng trống của từ trong bộ stopword bằng ký tự '_' để dễ xử lý hơn\r\n",
    "#stopwords = []\r\n",
    "for word in stopword:\r\n",
    "    stopword[x] = word.replace(' ', '_')\r\n",
    "    x+=1\r\n",
    "    #print(word)\r\n",
    "\r\n",
    "\r\n",
    "### LOAI BO STOPWORDS ###\r\n",
    "segss = []\r\n",
    "for a in segg:\r\n",
    "    tempseg = []\r\n",
    "    for b in a:\r\n",
    "        tempb = ''\r\n",
    "        for c in b:\r\n",
    "            #print(b)\r\n",
    "            if c == ' ':\r\n",
    "                #print(tempb)\r\n",
    "                if tempb not in stopword and tempb != '':\r\n",
    "                    tempseg.append(tempb)\r\n",
    "                if tempb == 'tốt': print(tempseg)\r\n",
    "                tempb = ''\r\n",
    "            else:\r\n",
    "                tempb += c\r\n",
    "        #tempseg.append(tempb)\r\n",
    "    if tempseg != []:\r\n",
    "        segss.append(tempseg)\r\n",
    "    i += 1\r\n",
    "#print(segss[:5])\r\n",
    "\r\n",
    "#Duyệt tất cả các tính từ vào bảng TABLE_ADJ\r\n",
    "TABLE_ADJ = []\r\n",
    "KiemTra = []\r\n",
    "Count_All_Adjs = 0\r\n",
    "count = 0\r\n",
    "for i in segss:\r\n",
    "    adj = dict()\r\n",
    "    cau = ''\r\n",
    "    for b in i:\r\n",
    "        cau += b + ' '\r\n",
    "    a = pos_tag(cau)\r\n",
    "    count += 1\r\n",
    "    #print(a[0][1])\r\n",
    "    for b in pos_tag(cau):\r\n",
    "        if b[1] == 'A':\r\n",
    "            if b[0] not in KiemTra:\r\n",
    "                adj['WORD'] = b[0]\r\n",
    "                KiemTra.append(b[0])\r\n",
    "                clear_output(wait=True)\r\n",
    "                print('Xét câu', count, ':', cau)\r\n",
    "                print('Loại từ: ', b[1], ', Từ: ', b[0])\r\n",
    "                #print(KiemTra)\r\n",
    "                print('Số lượng tính từ đã xét:', Count_All_Adjs)\r\n",
    "                print('Số lượng tính từ khác nhau trong bảng: ', len(TABLE_ADJ))\r\n",
    "                similar_words_1 = SYN.loc[SYN['Word1'] == b[0]]\r\n",
    "                similar_words_2 = SYN.loc[SYN['Word2'] == b[0]]\r\n",
    "                adj['COUNT'] = 1\r\n",
    "                if similar_words_1.empty != True:\r\n",
    "                    adj['SIMILAR'] = similar_words_1['Word2'].unique()[0]\r\n",
    "                    adj['COUNT'] += 1\r\n",
    "                    KiemTra.append(adj['SIMILAR'])\r\n",
    "            \r\n",
    "                if similar_words_2.empty != True:\r\n",
    "                    adj['SIMILAR'] = similar_words_2['Word1'].unique()[0]\r\n",
    "                    adj['COUNT'] += 1\r\n",
    "                    KiemTra.append(adj['SIMILAR'])\r\n",
    "                    \r\n",
    "                else:\r\n",
    "                    adj['SIMILAR'] = ''\r\n",
    "                if adj not in TABLE_ADJ:\r\n",
    "                    TABLE_ADJ.append(adj)\r\n",
    "                    #print(TABLE_ADJ)\r\n",
    "            else:\r\n",
    "                for x in TABLE_ADJ:\r\n",
    "                    if x['WORD'] == b[0]:\r\n",
    "                        x['COUNT'] += 1\r\n",
    "                    elif x['SIMILAR'] == b[0]:\r\n",
    "                        x['COUNT'] += 1\r\n",
    "            Count_All_Adjs += 1\r\n",
    "print('Số lượng tính từ đã xét xong: ', Count_All_Adjs)\r\n",
    "TABLE_ADJ.append(Count_All_Adjs)\r\n",
    "#print(TABLE_ADJ)\r\n",
    "#print(TABLE_ADJ[:5])\r\n",
    "#print(segss[:4])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Xét câu 116 : trợ gửi hoá_đơn mua hàng siêu đọc nhắn thèm phản phục_vụ kém trả_lời \n",
      "Loại từ:  A , Từ:  kém\n",
      "Số lượng tính từ đã xét: 80\n",
      "Số lượng tính từ khác nhau trong bảng:  19\n",
      "Số lượng tính từ đã xét xong:  82\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#Thứ tự chạy: 8\r\n",
    "\r\n",
    "#Kiểm tra độ tin tưởng của từng review dựa trên bảng tính từ:\r\n",
    "import numpy as np\r\n",
    "def Review_score(TatCaReviewDaXuLy, TABLE_ADJ, review, index, rating_star):\r\n",
    "    score = 0\r\n",
    "    constrain_2tt = 0\r\n",
    "    constrain_2tt_trung = 0\r\n",
    "    if abs(rating_star - TatCaReviewDaXuLy[index]['Sao']) <= 2:\r\n",
    "        score += 0.2\r\n",
    "    else: score += 0.1\r\n",
    "    if clf.predict([review])[0] == good_bad:\r\n",
    "        score += 0.1*good_bad\r\n",
    "    else:\r\n",
    "        score += 0.1*(1-good_bad)\r\n",
    "    for word in pos_tag(review):\r\n",
    "        if word[1] == 'N':\r\n",
    "            constrain_2tt += 1\r\n",
    "        if word[1] == 'A':\r\n",
    "            constrain_2tt += 1\r\n",
    "            if constrain_2tt == 2:\r\n",
    "                score += 0.1\r\n",
    "            for x in TABLE_ADJ[:-1]:\r\n",
    "                if x['WORD'] == word[0]:\r\n",
    "                    constrain_2tt_trung += 1\r\n",
    "                    if constrain_2tt_trung == 2:\r\n",
    "                        score += 0.1\r\n",
    "                    score += x['COUNT']/TABLE_ADJ[-1]*0.5\r\n",
    "    return score\r\n",
    "mini = 1\r\n",
    "maxi = 0\r\n",
    "All_review_score = []\r\n",
    "idx = 0\r\n",
    "idx_min = idx_max = 0\r\n",
    "cnt = 1\r\n",
    "for i in segss:\r\n",
    "    cau = ''\r\n",
    "    for b in i:\r\n",
    "        cau += b + ' '\r\n",
    "        score = Review_score(seg, TABLE_ADJ, cau, idx, rating_star)\r\n",
    "        if score > 1:\r\n",
    "            score = 1\r\n",
    "        if score < 0: score = 0\r\n",
    "    if score <= mini: \r\n",
    "        mini = score\r\n",
    "        idx_min = cnt - 1\r\n",
    "    if score >= maxi and score < 0.9: \r\n",
    "        maxi = score\r\n",
    "        idx_max = cnt - 1\r\n",
    "    clear_output(wait=True)\r\n",
    "    print('Sao trung bình:', rating_star)\r\n",
    "    print('Tính điểm cho review', cnt, ':', score)\r\n",
    "    print('Điểm thấp nhất:', mini, '\\nReview:', data[idx_min]['Nhan xet'], '\\nSao:', data[idx_min]['Sao'], '\\nĐiểm cao nhất:', maxi, '\\nReview:', data[idx_max]['Nhan xet'], '\\nSao:', data[idx_max]['Sao'])\r\n",
    "    cnt += 1\r\n",
    "    All_review_score.append(score)\r\n",
    "    idx += 1\r\n",
    "print(All_review_score[:30])\r\n",
    "#print(All_review_score[int(input('Nhập thứ tự câu:'))])\r\n",
    "np_review = np.array(All_review_score)\r\n",
    "#print(All_review_score)\r\n",
    "#print(seg[:6])\r\n",
    "idx = np.argpartition(np_review, -4)[-4:]\r\n",
    "#print(idx)\r\n",
    "print('Các review có điểm cao nhất:')\r\n",
    "for i in idx:\r\n",
    "    print(data[i]['Nhan xet'], ' score: ', All_review_score[i])\r\n",
    "    #print('Score:',All_review_score[i])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sao trung bình: 4.879432624113475\n",
      "Tính điểm cho review 121 : 0.42439024390243907\n",
      "Điểm thấp nhất: 0.2 \n",
      "Review: Sản phẩm tốt. Giao hàng nhanh! Mua đc flash sale nên giá khá rẻ! Sẽ ủng hộ các sản phẩm khác cho shop! \n",
      "Sao: 5 \n",
      "Điểm cao nhất: 0.5158536585365854 \n",
      "Review: bự hơn mình tưởng, xác nhận hơi lâu nhưng xứng đáng để chờ đợi nhe \n",
      "Sao: 5\n",
      "[0.30000000000000004, 0.3243902439024391, 0.2, 0.2, 0.2, 0.31829268292682933, 0.31219512195121957, 0.30000000000000004, 0.30000000000000004, 0.20609756097560977, 0.24268292682926831, 0.30000000000000004, 0.30000000000000004, 0.42439024390243907, 0.31219512195121957, 0.2, 0.2, 0.2, 0.21829268292682927, 0.30000000000000004, 0.2304878048780488, 0.22439024390243903, 0.26707317073170733, 0.30000000000000004, 0.2, 0.31829268292682933, 0.44268292682926835, 0.39146341463414636, 0.2, 0.2]\n",
      "Các review có điểm cao nhất:\n",
      "Giao hàng siêu tốc chỉ sau 3 ngày không tính ngày đặt đã nhận được hàng dù đang khó khăn mùa dịch. Mọi thứ đều đẹp và nguyên vẹn.  score:  0.44268292682926835\n",
      "Sản phẩm chính hãng tốt, chua active, minh mua sales 8.8 nên chi 13,2tr, gia rât tôt, giao cung nhanh  score:  0.4487804878048781\n",
      "Đợi hàng rất lâu mn ạ. Huhu. Mất 5 ngày í. Còn ipad chuẩn luôn nhé. Gói cx rất cẩn thận. Box to lắm khá ok nhé  score:  0.4548780487804878\n",
      "bự hơn mình tưởng, xác nhận hơi lâu nhưng xứng đáng để chờ đợi nhe  score:  0.5158536585365854\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}