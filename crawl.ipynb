{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Thá»© tá»± cháº¡y: 1\r\n",
    "\r\n",
    "#Train model nhan dien cam xuc trong cau#\r\n",
    "\r\n",
    "# -*- coding: utf-8 -*-\r\n",
    "from __future__ import print_function\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "# from sklearn.neural_network import MLPClassifier\r\n",
    "# from sklearn.tree import DecisionTreeClassifier\r\n",
    "# from sklearn.naive_bayes import MultinomialNB\r\n",
    "# from sklearn.linear_model import SGDClassifier\r\n",
    "# from sklearn.neighbors import KNeighborsClassifier\r\n",
    "# from sklearn.ensemble import AdaBoostClassifier\r\n",
    "# from sklearn.ensemble import RandomForestClassifier\r\n",
    "import pandas as pd\r\n",
    "from pyvi import ViTokenizer\r\n",
    "import re\r\n",
    "import string\r\n",
    "import codecs\r\n",
    "\r\n",
    "#Tá»« Ä‘iá»ƒn tÃ­ch cá»±c, tiÃªu cá»±c, phá»§ Ä‘á»‹nh\r\n",
    "path_nag = './sentiment/sentiment_dicts/nag.txt'\r\n",
    "path_pos = './sentiment/sentiment_dicts/pos.txt'\r\n",
    "path_not = './sentiment/sentiment_dicts/not.txt'\r\n",
    "\r\n",
    "with codecs.open(path_nag, 'r', encoding='UTF-8') as f:\r\n",
    "    nag = f.readlines()\r\n",
    "nag_list = [n.replace('\\n', '') for n in nag]\r\n",
    "\r\n",
    "with codecs.open(path_pos, 'r', encoding='UTF-8') as f:\r\n",
    "    pos = f.readlines()\r\n",
    "pos_list = [n.replace('\\n', '') for n in pos]\r\n",
    "with codecs.open(path_not, 'r', encoding='UTF-8') as f:\r\n",
    "    not_ = f.readlines()\r\n",
    "not_list = [n.replace('\\n', '') for n in not_]\r\n",
    "\r\n",
    "\r\n",
    "VN_CHARS_LOWER = u'áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°'\r\n",
    "VN_CHARS_UPPER = u'áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ'\r\n",
    "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\r\n",
    "def no_marks(s):\r\n",
    "    __INTAB = [ch for ch in VN_CHARS]\r\n",
    "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\r\n",
    "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\r\n",
    "    __r = re.compile(\"|\".join(__INTAB))\r\n",
    "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\r\n",
    "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\r\n",
    "    return result\r\n",
    "\r\n",
    "def normalize_text(text):\r\n",
    "\r\n",
    "    #Remove cÃ¡c kÃ½ tá»± kÃ©o dÃ i: vd: Ä‘áº¹ppppppp\r\n",
    "    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\r\n",
    "\r\n",
    "    # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng\r\n",
    "    text = text.lower()\r\n",
    "\r\n",
    "    #Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯\r\n",
    "    replace_list = {\r\n",
    "        'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',\r\n",
    "        'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ', 'uáº£': 'á»§a',\r\n",
    "        'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘','Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',\r\n",
    "        'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»','ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',\r\n",
    "        'eÌ‰': 'áº»', 'Ã k': u' Ã  ','aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯','Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',\r\n",
    "        #Quy cÃ¡c icon vá» 2 loáº¡i emoj: TÃ­ch cá»±c hoáº·c tiÃªu cá»±c\r\n",
    "        \"ğŸ‘¹\": \"nagative\", \"ğŸ‘»\": \"positive\", \"ğŸ’ƒ\": \"positive\",'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ',\r\n",
    "        \"ğŸ’„\": \"positive\", \"ğŸ’\": \"positive\", \"ğŸ’©\": \"positive\",\"ğŸ˜•\": \"nagative\", \"ğŸ˜±\": \"nagative\", \"ğŸ˜¸\": \"positive\",\r\n",
    "        \"ğŸ˜¾\": \"nagative\", \"ğŸš«\": \"nagative\",  \"ğŸ¤¬\": \"nagative\",\"ğŸ§š\": \"positive\", \"ğŸ§¡\": \"positive\",'ğŸ¶':' positive ',\r\n",
    "        'ğŸ‘': ' nagative ', 'ğŸ˜£': ' nagative ','âœ¨': ' positive ', 'â£': ' positive ','â˜€': ' positive ',\r\n",
    "        'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' positive ',\r\n",
    "        'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' nagative ', 'ğŸ˜¢': ' nagative ',\r\n",
    "        'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' nagative ', 'ğŸ˜Š': ' positive ',\r\n",
    "        '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' nagative ', 'ğŸ˜­': ' nagative ',\r\n",
    "        'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',\r\n",
    "        '^^': ' positive ', 'ğŸ˜¨': ' nagative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',\r\n",
    "        'ğŸ˜–': ' nagative ', 'ğŸ˜€': ' positive ', ':((': ' nagative ', 'ğŸ˜¡': ' nagative ', 'ğŸ˜ ': ' nagative ',\r\n",
    "        'ğŸ˜’': ' nagative ', 'ğŸ™‚': ' positive ', 'ğŸ˜': ' nagative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',\r\n",
    "        'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' nagative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',\r\n",
    "        'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' nagative ', 'ğŸ˜“': ' nagative ', 'ï¸ğŸ†—ï¸': ' positive ',\r\n",
    "        'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ',\r\n",
    "        'ğŸ’“': ' positive ', 'ğŸ˜': ' nagative ', ':3': ' positive ', 'ğŸ˜«': ' nagative ', 'ğŸ˜¥': ' nagative ',\r\n",
    "        'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',\r\n",
    "        'ğŸ˜—': ' positive ', 'ğŸ¤”': ' nagative ', 'ğŸ˜‘': ' nagative ', 'ğŸ”¥': ' nagative ', 'ğŸ™': ' nagative ',\r\n",
    "        'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',\r\n",
    "        'ğŸ˜š': ' positive ', 'âŒ': ' nagative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',\r\n",
    "        'ğŸŒ': ' positive ',  'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',\r\n",
    "        'ğŸŒ¼': ' positive ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',\r\n",
    "        'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ',\r\n",
    "        'ğŸ’°': ' positive ',  'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ',\r\n",
    "        'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ','â˜¹': ' nagative ',  'ğŸ’€': ' nagative ',\r\n",
    "        'ğŸ˜”': ' nagative ', 'ğŸ˜§': ' nagative ', 'ğŸ˜©': ' nagative ', 'ğŸ˜°': ' nagative ', 'ğŸ˜³': ' nagative ',\r\n",
    "        'ğŸ˜µ': ' nagative ', 'ğŸ˜¶': ' nagative ', 'ğŸ™': ' nagative ',\r\n",
    "        #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words\r\n",
    "        ':))': '  positive ', ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',\r\n",
    "        'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',\r\n",
    "        ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',\r\n",
    "        'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ',\r\n",
    "        'kg ': u' khÃ´ng ','not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '\"k ': u' khÃ´ng ',' kh ':u' khÃ´ng ','kÃ´':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '\"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',\r\n",
    "        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',\r\n",
    "        ' lol ': ' nagative ',' cc ': ' nagative ','cute': u' dá»… thÆ°Æ¡ng ','huhu': ' nagative ', ' vs ': u' vá»›i ', 'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',\r\n",
    "        ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',\r\n",
    "        'Ä‘c': u' Ä‘Æ°á»£c ','authentic': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' positive ', 'store': u' cá»­a hÃ ng ',\r\n",
    "        'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ','god': u' tá»‘t ','wel done':' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',\r\n",
    "        'sáº¥u': u' xáº¥u ','gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': u' bÃ¬nh thÆ°á»ng ',\r\n",
    "        'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',\r\n",
    "        'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng','chat':' cháº¥t ', 'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡','fresh': ' tÆ°Æ¡i ','sad': ' tá»‡ ',\r\n",
    "        'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',\r\n",
    "        'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',\r\n",
    "        'cháº¥t lg': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u'bjo':u' bao giá» ',\r\n",
    "        'thik': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u'quáº£ ng ':u' quáº£ng  ',\r\n",
    "        'dep': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ','delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ',\r\n",
    "        'iu': u' yÃªu ','fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' positive ',\r\n",
    "        ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', 'ib':u' nháº¯n tin ', 'rep':u' tráº£ lá»i ',u'fback':' feedback ','fedback':' feedback ',\r\n",
    "        #dÆ°á»›i 3* quy vá» 1*, trÃªn 3* quy vá» 5*\r\n",
    "        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\r\n",
    "        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\r\n",
    "        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}\r\n",
    "\r\n",
    "    for k, v in replace_list.items():\r\n",
    "        text = text.replace(k, v)\r\n",
    "\r\n",
    "    # chuyen punctuation thÃ nh space\r\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\r\n",
    "    text = text.translate(translator)\r\n",
    "\r\n",
    "    text = ViTokenizer.tokenize(text)\r\n",
    "    texts = text.split()\r\n",
    "    len_text = len(texts)\r\n",
    "\r\n",
    "    texts = [t.replace('_', ' ') for t in texts]\r\n",
    "    for i in range(len_text):\r\n",
    "        cp_text = texts[i]\r\n",
    "        if cp_text in not_list: # Xá»­ lÃ½ váº¥n Ä‘á» phá»§ Ä‘á»‹nh (VD: Ã¡o nÃ y cháº³ng Ä‘áº¹p--> Ã¡o nÃ y notpos)\r\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\r\n",
    "\r\n",
    "            for j in range(numb_word):\r\n",
    "                if texts[i + j + 1] in pos_list:\r\n",
    "                    texts[i] = 'notpos'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "\r\n",
    "                if texts[i + j + 1] in nag_list:\r\n",
    "                    texts[i] = 'notnag'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "        else: #ThÃªm feature cho nhá»¯ng sentiment words (Ã¡o nÃ y Ä‘áº¹p--> Ã¡o nÃ y Ä‘áº¹p positive)\r\n",
    "            if cp_text in pos_list:\r\n",
    "                texts.append('positive')\r\n",
    "            elif cp_text in nag_list:\r\n",
    "                texts.append('nagative')\r\n",
    "\r\n",
    "    text = u' '.join(texts)\r\n",
    "\r\n",
    "    #remove ná»‘t nhá»¯ng kÃ½ tá»± thá»«a thÃ£i\r\n",
    "    text = text.replace(u'\"', u' ')\r\n",
    "    text = text.replace(u'ï¸', u'')\r\n",
    "    text = text.replace('ğŸ»','')\r\n",
    "    return text\r\n",
    "\r\n",
    "#print(normalize_text('Nma kÃ­nh Ä‘áº¹p cháº¯c cháº¯n ğŸ˜'))\r\n",
    "\r\n",
    "class DataSource(object):\r\n",
    "\r\n",
    "    def _load_raw_data(self, filename, is_train=True):\r\n",
    "\r\n",
    "        a = []\r\n",
    "        b = []\r\n",
    "\r\n",
    "        regex = 'train_'\r\n",
    "        if not is_train:\r\n",
    "            regex = 'test_'\r\n",
    "\r\n",
    "        with open(filename, 'r', encoding='UTF-8') as file:\r\n",
    "            for line in file:\r\n",
    "                if regex in line:\r\n",
    "                    b.append(a)\r\n",
    "                    a = [line]\r\n",
    "                elif line != '\\n':\r\n",
    "                    a.append(line)\r\n",
    "        b.append(a)\r\n",
    "\r\n",
    "        return b[1:]\r\n",
    "\r\n",
    "    def _create_row(self, sample, is_train=True):\r\n",
    "\r\n",
    "        d = {}\r\n",
    "        d['id'] = sample[0].replace('\\n', '')\r\n",
    "        review = \"\"\r\n",
    "\r\n",
    "        if is_train:\r\n",
    "            for clause in sample[1:-1]:\r\n",
    "                review += clause.replace('\\n', ' ')\r\n",
    "                review = review.replace('.', ' ')\r\n",
    "\r\n",
    "            d['label'] = int(sample[-1].replace('\\n', ' '))\r\n",
    "        else:\r\n",
    "            for clause in sample[1:]:\r\n",
    "                review += clause.replace('\\n', ' ')\r\n",
    "                review = review.replace('.', ' ')\r\n",
    "\r\n",
    "\r\n",
    "        d['review'] = review\r\n",
    "\r\n",
    "        return d\r\n",
    "\r\n",
    "    def load_data(self, filename, is_train=True):\r\n",
    "\r\n",
    "        raw_data = self._load_raw_data(filename, is_train)\r\n",
    "        lst = []\r\n",
    "\r\n",
    "        for row in raw_data:\r\n",
    "            lst.append(self._create_row(row, is_train))\r\n",
    "\r\n",
    "        return lst\r\n",
    "\r\n",
    "    def transform_to_dataset(self, x_set,y_set):\r\n",
    "        X, y = [], []\r\n",
    "        for document, topic in zip(list(x_set), list(y_set)):\r\n",
    "            document = normalize_text(document)\r\n",
    "            X.append(document.strip())\r\n",
    "            y.append(topic)\r\n",
    "            #Augmentation báº±ng cÃ¡ch remove dáº¥u tiáº¿ng Viá»‡t\r\n",
    "            X.append(no_marks(document))\r\n",
    "            y.append(topic)\r\n",
    "        return X, y\r\n",
    "\r\n",
    "ds = DataSource()\r\n",
    "train_data = pd.DataFrame(ds.load_data('./sentiment/data_clean/train.crash'))\r\n",
    "new_data = []\r\n",
    "\r\n",
    "#ThÃªm máº«u báº±ng cÃ¡ch láº¥y trong tá»« Ä‘iá»ƒn Sentiment (nag/pos)\r\n",
    "for index,row in enumerate(nag_list):\r\n",
    "    new_data.append(['pos'+str(index),'0',row])\r\n",
    "for index,row in enumerate(nag_list):\r\n",
    "    new_data.append(['nag'+str(index),'1',row])\r\n",
    "\r\n",
    "new_data = pd.DataFrame(new_data,columns=list(['id','label','review']))\r\n",
    "train_data.append(new_data)\r\n",
    "test_data = pd.DataFrame(ds.load_data('./sentiment/data_clean/test.crash', is_train=False))\r\n",
    "\r\n",
    "#Try some models\r\n",
    "classifiers = [\r\n",
    "            # MultinomialNB(),\r\n",
    "            # DecisionTreeClassifier(),\r\n",
    "            # LogisticRegression(),\r\n",
    "            # SGDClassifier(),\r\n",
    "            LinearSVC(fit_intercept = True,multi_class='crammer_singer', C=1),\r\n",
    "        ]\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.review, train_data.label, test_size=0.3,random_state=42)\r\n",
    "X_train, y_train = ds.transform_to_dataset(X_train,y_train)\r\n",
    "X_test, y_test = ds.transform_to_dataset(X_test, y_test)\r\n",
    "\r\n",
    "#THÃŠM STOPWORD LÃ€ NHá»®NG Tá»ª KÃ‰M QUAN TRá»ŒNG\r\n",
    "stop_ws = (u'ráº±ng',u'thÃ¬',u'lÃ ',u'mÃ ')\r\n",
    "\r\n",
    "for classifier in classifiers:\r\n",
    "    steps = []\r\n",
    "    steps.append(('CountVectorizer', CountVectorizer(ngram_range=(1,5),stop_words=stop_ws,max_df=0.5, min_df=5)))\r\n",
    "    steps.append(('tfidf', TfidfTransformer(use_idf=False, sublinear_tf = True,norm='l2',smooth_idf=True)))\r\n",
    "    steps.append(('classifier', classifier))\r\n",
    "    clf = Pipeline(steps)\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "    y_pred = clf.predict(X_test)\r\n",
    "    report1 = metrics.classification_report(y_test, y_pred, labels=[1,0], digits=3)\r\n",
    "\r\n",
    "X_train, y_train = ds.transform_to_dataset(train_data.review, train_data.label)\r\n",
    "\r\n",
    "\r\n",
    "#TRAIN OVERFITTING/ERRO ANALYSIS\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "y_pred = clf.predict(X_train)\r\n",
    "report2 = metrics.classification_report(y_train, y_pred, labels=[1,0], digits=3)\r\n",
    "\r\n",
    "#ERRO ANALYSIS\r\n",
    "#for id,x, y1, y2 in zip(train_data.id, X_train, y_train, y_pred):\r\n",
    "    #if y1 != y2:\r\n",
    "        # CHECK EACH WRONG SAMPLE POSSITIVE/NAGATIVE\r\n",
    "        #if y1!=1:#0:\r\n",
    "            #print(id,x, y1, y2)\r\n",
    "\r\n",
    "#CROSS VALIDATION\r\n",
    "cross_score = cross_val_score(clf, X_train,y_train, cv=5)\r\n",
    "\r\n",
    "#REPORT\r\n",
    "print('DATASET LEN %d'%(len(X_train)))\r\n",
    "print('TRAIN 70/30 \\n\\n',report1)\r\n",
    "print('TRAIN OVERFITING\\n\\n',report2)\r\n",
    "print(\"CROSSVALIDATION 5 FOLDS: %0.4f (+/- %0.4f)\" % (cross_score.mean(), cross_score.std() * 2))\r\n",
    "\r\n",
    "# #SAVE FILE SUBMIT\r\n",
    "#test_list = []\r\n",
    "#for document in test_data.review:\r\n",
    "#    document = normalize_text(document)\r\n",
    "#    test_list.append(document)\r\n",
    "#print(test_list)\r\n",
    "#y_predict = clf.predict(test_list)\r\n",
    "#test_data['label'] = y_predict\r\n",
    "# test_data['content'] = test_list\r\n",
    "#test_data = test_data.sort_values(by=['label'])\r\n",
    "#test_data[['id', 'label']].to_csv('submit.csv', index=False)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:928: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:928: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DATASET LEN 32146\n",
      "TRAIN 70/30 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.938     0.945     0.941      4438\n",
      "           0      0.952     0.947     0.950      5206\n",
      "\n",
      "    accuracy                          0.946      9644\n",
      "   macro avg      0.945     0.946     0.946      9644\n",
      "weighted avg      0.946     0.946     0.946      9644\n",
      "\n",
      "TRAIN OVERFITING\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.981     0.985     0.983     14766\n",
      "           0      0.987     0.984     0.985     17380\n",
      "\n",
      "    accuracy                          0.984     32146\n",
      "   macro avg      0.984     0.984     0.984     32146\n",
      "weighted avg      0.984     0.984     0.984     32146\n",
      "\n",
      "CROSSVALIDATION 5 FOLDS: 0.9436 (+/- 0.0096)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Thá»© tá»± cháº¡y: 2\r\n",
    "#Pháº§n lá»c itemID vÃ  shopID ra tá»« link\r\n",
    "def getID(URL):\r\n",
    "    group = ''\r\n",
    "    itemID = shopID = ''\r\n",
    "    chk = ''\r\n",
    "    cnt = 0\r\n",
    "    isdone = False\r\n",
    "    for i in URL:\r\n",
    "        if chk != 'i.':\r\n",
    "            if i != '-':\r\n",
    "                group += i\r\n",
    "                if group == 'i.':\r\n",
    "                    chk = group\r\n",
    "            else: group = ''\r\n",
    "        else:\r\n",
    "            if isdone == False:\r\n",
    "                if i != '.':\r\n",
    "                    shopID += i\r\n",
    "                else: isdone = True\r\n",
    "            else:\r\n",
    "                if i in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\r\n",
    "                    itemID += i\r\n",
    "                else: return itemID, shopID\r\n",
    "    return itemID, shopID\r\n",
    "print(getID('https://shopee.vn/KÃ­nh-cáº­n-Ä‘á»•i-mÃ u-máº¯t-kÃ­nh-nam-cáº­n-rÃ¢m-2-in1-máº¯t-cáº­n-Ä‘á»•i-mÃ u-theo-Ã¡nh-náº¯ng-i.18722862.5560946376?adsid=0&campaignid=0&position=1'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('5560946376', '18722862')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Thá»© tá»± cháº¡y: 3\r\n",
    "\r\n",
    "#Láº¥y cÃ¡c review tá»« itemID vÃ  shopID Ä‘Ã£ cÃ³ á»Ÿ trÃªn thÃ´ng qua API Shopee\r\n",
    "import requests, time\r\n",
    "from IPython.display import display, clear_output\r\n",
    "\r\n",
    "\r\n",
    "headers = {\r\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\"\r\n",
    "}\r\n",
    "\r\n",
    "if (int(input('Chá»n 1 Ä‘á»ƒ nháº­p id, chá»n 2 Ä‘á»ƒ nháº­p URL:')) == 1):\r\n",
    "    itemID = input('Nháº­p id item:')\r\n",
    "    shopID = input('Nháº­p id shop:')\r\n",
    "else:\r\n",
    "    productURL = input('Nhap URL san pham:')\r\n",
    "#itemID = input('Nhap id cua item:')\r\n",
    "#shopID = input('Nhap id cua shop:')\r\n",
    "\r\n",
    "    itemID, shopID = getID(productURL)\r\n",
    "\r\n",
    "\r\n",
    "path_not = './sentiment/sentiment_dicts/not.txt'\r\n",
    "with codecs.open(path_not, 'r', encoding='UTF-8') as f:\r\n",
    "    not_ = f.readlines()\r\n",
    "not_list = [n.replace('\\n', '') for n in not_]\r\n",
    "api = \"https://shopee.vn/api/v2/item/get_ratings?filter=0&flag=1&type=0&limit=6\"\r\n",
    "api += '&itemid=' + itemID + '&shopid=' + shopID\r\n",
    "r = requests.get(api).json()\r\n",
    "api = \"https://shopee.vn/api/v2/item/get?\" + 'itemid=' + itemID + '&shopid=' + shopID\r\n",
    "#print(api)\r\n",
    "r1 = requests.get(api, headers=headers).json()\r\n",
    "#print(r1)\r\n",
    "tensp = r1['item']['name']\r\n",
    "rating_star = 0\r\n",
    "\r\n",
    "\r\n",
    "rv_num = r['data']['item_rating_summary']['rating_total']\r\n",
    "rcount_text = r['data']['item_rating_summary']['rcount_with_context']\r\n",
    "data = []\r\n",
    "#data.append({'ID san pham:': itemID, 'ID shop:': shopID, 'Tong danh gia:': rv_num})\r\n",
    "offset_length = 0\r\n",
    "i = 1\r\n",
    "chk_empty = 0\r\n",
    "start = time.time()\r\n",
    "estimate_start = time.time()\r\n",
    "estimate_time = 0\r\n",
    "time_step = 0.5\r\n",
    "rv_count = 1\r\n",
    "rv_left = rcount_text\r\n",
    "while i < rcount_text:\r\n",
    "    if (time.time() - start > 6000):\r\n",
    "        break\r\n",
    "    if (time.time() - estimate_start > time_step):\r\n",
    "        estimate_time = rv_left/(rv_count/time_step)\r\n",
    "        rv_left -= rv_count\r\n",
    "        estimate_start = time.time()\r\n",
    "        rv_count = 1\r\n",
    "    api = \"https://shopee.vn/api/v2/item/get_ratings?filter=0&flag=1&type=0&limit=6\"\r\n",
    "    api += '&itemid=' + itemID + '&shopid=' + shopID + '&offset=' + str(offset_length)\r\n",
    "    r = requests.get(api).json()\r\n",
    "    if r['data']['ratings'] == []:\r\n",
    "        print(r)\r\n",
    "        break\r\n",
    "    #print(r['data']['ratings'])\r\n",
    "    clear_output(wait=True)\r\n",
    "    print('Æ¯á»›c tÃ­nh thá»i gian cÃ²n láº¡i:', estimate_time//60, 'phÃºt', estimate_time%60,'giÃ¢y')\r\n",
    "    print('Load review: ', i, ' offset:', offset_length)\r\n",
    "    for departure in r['data']['ratings']:\r\n",
    "        if departure['comment'] == '':\r\n",
    "            chk_empty += 1\r\n",
    "            continue\r\n",
    "        dem_tu = 0\r\n",
    "        for chu in departure['comment']:\r\n",
    "            if chu == ' ':\r\n",
    "                dem_tu += 1\r\n",
    "            if dem_tu >= 13:\r\n",
    "                break\r\n",
    "        if dem_tu < 13 or dem_tu >500: \r\n",
    "            offset_length += 1\r\n",
    "            continue\r\n",
    "        data.append({\r\n",
    "        'Number': i,\r\n",
    "        'Nguoi dung': departure['author_username'],\r\n",
    "        'Nhan xet': departure['comment'],\r\n",
    "        'Sao': departure['rating_star']\r\n",
    "        })\r\n",
    "        #print(estimate_time)\r\n",
    "        #print(\"\\nNumber: \", data[i-1]['Number'])\r\n",
    "        #print(\"Nguoi dung: \", data[i-1]['Nguoi dung'])\r\n",
    "        #print(\"Nhan xet: \", data[i-1]['Nhan xet'])\r\n",
    "        #print(\"Sao: \", data[i-1]['Sao'])\r\n",
    "        #print(\"-----------------\")\r\n",
    "\r\n",
    "        i += 1\r\n",
    "        rv_count += 1\r\n",
    "\r\n",
    "        offset_length += 1\r\n",
    "    if chk_empty > 5:\r\n",
    "        #i += 1\r\n",
    "        offset_length +=1\r\n",
    "        chk_empty = 0\r\n",
    "    #rv_num -= 6\r\n",
    "clear_output()\r\n",
    "up = 1\r\n",
    "for x in r['data']['item_rating_summary']['rating_count']:\r\n",
    "    rating_star += up * x\r\n",
    "    up += 1\r\n",
    "rating_star /= r['data']['item_rating_summary']['rating_total']\r\n",
    "print('Thá»i gian cháº¡y:', (time.time() - start)//60, 'phÃºt', (time.time() - start)%60,'giÃ¢y')\r\n",
    "print('Sao trung bÃ¬nh:', rating_star)\r\n",
    "print('TÃªn sáº£n pháº©m:', tensp)\r\n",
    "print('Load review:', i)\r\n",
    "print(\"ID san pham:\", itemID, \"\\nID shop:\", shopID, \"\\nTong danh gia:\", rv_num, \"\\nTong danh gia co binh luan:\", rcount_text)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thá»i gian cháº¡y: 0.0 phÃºt 51.61233448982239 giÃ¢y\n",
      "Sao trung bÃ¬nh: 4.879432624113475\n",
      "TÃªn sáº£n pháº©m: Apple iPad Air 4 10.9 inch Wi-Fi\n",
      "Load review: 150\n",
      "ID san pham: 5076952578 \n",
      "ID shop: 88201679 \n",
      "Tong danh gia: 423 \n",
      "Tong danh gia co binh luan: 231\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import unidecode\r\n",
    "\r\n",
    "def SentenceSpliter(sentence):\r\n",
    "    split = []\r\n",
    "    word = ''\r\n",
    "    for i in range(0, len(sentence)):\r\n",
    "        if sentence[i] == ' ':\r\n",
    "            split.append(word)\r\n",
    "            word = ''\r\n",
    "        elif i == len(sentence) - 1 and word != '':\r\n",
    "            word += sentence[i]\r\n",
    "            split.append(word)\r\n",
    "        else: word += sentence[i]\r\n",
    "    return split\r\n",
    "\r\n",
    "def SentenceCombiner(split):\r\n",
    "    sentence = ''\r\n",
    "    for i in range(0, len(split)):\r\n",
    "        if i != len(split)-1:\r\n",
    "            sentence += split[i] + ' '\r\n",
    "        else:\r\n",
    "            sentence += split[i]\r\n",
    "    return sentence\r\n",
    "\r\n",
    "def XoaDau(word, syllable):\r\n",
    "    rmv_accent = word\r\n",
    "    for x in rmv_accent:\r\n",
    "        for s in syllable:\r\n",
    "            if x != s:\r\n",
    "                #print(syllable[s])\r\n",
    "                for j in syllable[s]:\r\n",
    "                    if x == j:\r\n",
    "                        rmv_accent = rmv_accent.replace(x, s)\r\n",
    "    return rmv_accent\r\n",
    "\r\n",
    "def VanNguyenAm(word, syllable):\r\n",
    "    #Váº§n Ä‘Æ¡n giáº£n 1: chá»‰ chá»©a 1 nguyÃªn Ã¢m\r\n",
    "    correct = False\r\n",
    "    if len(word) == 1:\r\n",
    "            while (correct != True):\r\n",
    "                for x in syllable:\r\n",
    "                    if x in ['e', 'a', 'o', 'Ãª', 'Æ¡', 'Ã´', 'i', 'y', 'Æ°', 'u']:\r\n",
    "                        if word != x:\r\n",
    "                            for y in range(0, 5):\r\n",
    "                                if word == syllable[x][y]:\r\n",
    "                                    correct = True\r\n",
    "                        else:\r\n",
    "                            correct = True\r\n",
    "                    else: continue\r\n",
    "                break\r\n",
    "    return correct\r\n",
    "\r\n",
    "#Váº§n Ä‘Æ¡n giáº£n (2): NguyÃªn Ã¢m + phá»¥ Ã¢m cuá»‘i ~ Váº§n ngÆ°á»£c\r\n",
    "def VanDonGian2(word, syllable, vowel, consonant):\r\n",
    "    bangVanLoai2 = [[1, 1, 1, 1, 0, 1, 1, 1],\r\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0],\r\n",
    "                    [1, 0, 0, 1, 1, 0, 0, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [0, 1, 1, 1, 0, 1, 1, 1],\r\n",
    "                    [0, 0, 1, 1, 0, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1],\r\n",
    "                    [1, 0, 1, 1, 1, 0, 1, 1]]\r\n",
    "    correct = [False, False]\r\n",
    "    for j in syllable:\r\n",
    "        if word[0] == j:\r\n",
    "            x = vowel['singleVowel'].index(word[0])\r\n",
    "            correct[0] = True\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            for t in range(0, 5):\r\n",
    "                if word[0] == syllable[j][t]:\r\n",
    "                    x = vowel['singleVowel'].index(j)\r\n",
    "                    correct[0] = True\r\n",
    "                    break\r\n",
    "    if correct[0] == True:\r\n",
    "        if word[1:] in consonant['tConsonant']:\r\n",
    "            y = consonant['tConsonant'].index(word[1:])\r\n",
    "            correct[1] = True\r\n",
    "        if correct[1] == True:\r\n",
    "            if bangVanLoai2[x][y] != 1:\r\n",
    "                return False\r\n",
    "            return True\r\n",
    "    return False\r\n",
    "\r\n",
    "#Kiá»ƒm tra váº§n hÃ²a Ã¢m vÃ  há»£p Ã¢m\r\n",
    "def KtraHoaAmHopAm(word, syllable, consonant):\r\n",
    "    rmv_accent = word\r\n",
    "    tailConsonant = ''\r\n",
    "    chk = ''\r\n",
    "    for i in range(1,3):\r\n",
    "        chk += rmv_accent[i-3]\r\n",
    "        if chk in consonant['tConsonant']:\r\n",
    "            tailConsonant = chk\r\n",
    "            #rmv_accent.replace(rmv_accent[-1], '')\r\n",
    "        else: chk = ''\r\n",
    "    #print(tailConsonant)\r\n",
    "    for i in range(len(tailConsonant)):\r\n",
    "        rmv_accent = rmv_accent.rstrip(rmv_accent[-1])\r\n",
    "    #print(rmv_accent)\r\n",
    "    if rmv_accent in ['eo', 'ao', 'ai', 'oe', 'oa', 'oi', 'Ãªu', 'Æ¡i', 'Ã´i', 'ia',\r\n",
    "    'iu', 'Æ°a', 'Æ°i', 'Æ°u', 'ua', 'uÃª', 'ui', 'uy']:\r\n",
    "        return True\r\n",
    "\r\n",
    "    if rmv_accent in ['ay', 'Ã¢y', 'au', 'Ã¢u']:\r\n",
    "        if tailConsonant == '':\r\n",
    "            return True\r\n",
    "        return False\r\n",
    "    if rmv_accent in ['oÄƒ', 'uÃ¢', 'iÃª', 'yÃª', 'Æ°Æ¡', 'uÃ´']:\r\n",
    "        if tailConsonant != '':\r\n",
    "            return True\r\n",
    "    if rmv_accent in ['uÃ´i', 'Æ°Æ¡i', 'Æ°Æ¡u','yÃªu', 'iÃªu']:\r\n",
    "        return True\r\n",
    "    return False\r\n",
    "\r\n",
    "def SentenceCorrector(splits):\r\n",
    "    #split cÃ³ dáº¡ng: split = ['tá»«', 'vÃ­, 'dá»¥'...]\r\n",
    "    #singleVowel: NguyÃªn Ã¢m Ä‘Æ¡n -> CÃ³ thá»ƒ Ä‘á»©ng 1 mÃ¬nh hoáº·c ghÃ©p vá»›i nhá»¯ng chá»¯ khÃ¡c\r\n",
    "    #sVowel: BÃ¡n nguyÃªn Ã¢m -> KhÃ´ng thá»ƒ Ä‘á»©ng 1 mÃ¬nh hay Ä‘á»©ng cuá»‘i mÃ  pháº£i cÃ³ phá»¥ Ã¢m cuá»‘i Ä‘i theo\r\n",
    "    #hConsonant: Phá»¥ Ã¢m Ä‘áº§u: Ä‘á»©ng trÆ°á»›c nguyÃªn Ã¢m\r\n",
    "    #tConsonant: Phá»¥ Ã¢m cuá»‘i: Ä‘á»©ng sau nguyÃªn Ã¢m\r\n",
    "    \r\n",
    "    vowel = {'singleVowel':['i', 'y', 'Æ°', 'u', 'Ãª', 'Æ¡', 'Ã¢', 'Ã´', 'e', 'a', 'Äƒ', 'o'],\r\n",
    "    'sVowel':['Äƒ', 'Ã¢', 'iÃª', 'yÃª', 'uÃ´', 'Æ°Æ¡']}\r\n",
    "    consonant = {'hConsonant':['b', 'c', 'd', 'Ä‘', 'g', 'gh', 'h', 'k', 'l', 'm',\r\n",
    "    'n', 'r', 's', 't', 'v', 'x', 'ch' , 'gi', 'kh', 'nh', 'ng', 'ngh', 'qu', 'ph',\r\n",
    "    'th', 'tr'], \r\n",
    "    'tConsonant': ['c', 'ch', 'm', 'n', 'ng', 'nh', 'p', 't']}\r\n",
    "    syllable = {'i':['Ã­', 'Ã¬', 'á»‰', 'Ä©', 'á»‹'], 'y': ['Ã½', 'á»³', 'á»·', 'á»¹', 'á»µ'],\r\n",
    "    'Æ°': ['á»©', 'á»«', 'á»­', 'á»¯', 'á»±'], 'u': ['Ãº', 'Ã¹', 'á»§', 'Å©', 'á»¥'],\r\n",
    "    'Ãª': ['áº¿', 'á»', 'á»ƒ', 'á»…', 'á»‡'], 'Æ¡': ['á»›', 'á»', 'á»Ÿ', 'á»¡', 'á»£'],\r\n",
    "    'Ã¢': ['áº¥', 'áº§', 'áº©', 'áº«', 'áº­'], 'Ã´': ['á»‘', 'á»“', 'á»•', 'á»—', 'á»™'],\r\n",
    "    'e': ['Ã©', 'Ã¨', 'áº»', 'áº½', 'áº¹'], 'a': ['Ã¡', 'Ã ', 'áº£', 'Ã£', 'áº¡'],\r\n",
    "    'Äƒ': ['áº¯', 'áº±', 'áº³', 'áºµ', 'áº·'], 'o': ['Ã³', 'Ã²', 'á»', 'Ãµ', 'á»']}\r\n",
    "    rmv = -1\r\n",
    "\r\n",
    "    split = splits.copy()\r\n",
    "    for i in splits:\r\n",
    "        rmv +=1\r\n",
    "    #Ktra Ä‘iá»u kiá»‡n: Tá»« = váº§n NguyÃªn Ã‚m:\r\n",
    "        if (len(i)==1):\r\n",
    "            #print('1 tu:', i)\r\n",
    "            if VanNguyenAm(i, syllable) != True:\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "        else:\r\n",
    "            #TÃ¡ch phá»¥ Ã¢m Ä‘áº§u\r\n",
    "            #pad = True\r\n",
    "            headConsonant = ''\r\n",
    "            chk = ''\r\n",
    "            cnt = 0\r\n",
    "            for y in i:\r\n",
    "                cnt += 1\r\n",
    "                chk += y\r\n",
    "                if chk in consonant['hConsonant']:\r\n",
    "                    headConsonant = chk\r\n",
    "                else:\r\n",
    "                    if cnt > 1: break\r\n",
    "                #if headConsonant == '': \r\n",
    "                #    split.pop(rmv)\r\n",
    "                #    pad = False\r\n",
    "                #    break\r\n",
    "            #if pad == False:\r\n",
    "            #    continue\r\n",
    "            #print('tÃ¡ch phá»¥ Ã¢m Ä‘áº§u: ', headConsonant)\r\n",
    "            i = i.replace(headConsonant, '')\r\n",
    "            i = XoaDau(i, syllable)\r\n",
    "            #print(i)\r\n",
    "            if (len(i)<1):\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -=1\r\n",
    "                continue\r\n",
    "            if (len(i) == 1):\r\n",
    "                #print('TH1:', i)\r\n",
    "                if VanNguyenAm(i, syllable) == True:\r\n",
    "                    continue\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "                continue\r\n",
    "            if VanDonGian2(i, syllable, vowel, consonant) == True:\r\n",
    "                #print('TH2:', i)\r\n",
    "                continue\r\n",
    "            elif KtraHoaAmHopAm(i, syllable, consonant) == True:\r\n",
    "                #print('TH3:', i)\r\n",
    "                continue\r\n",
    "            else: \r\n",
    "                #print('TH5:', i)\r\n",
    "                split.pop(rmv)\r\n",
    "                rmv -= 1\r\n",
    "    return split\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "sentence = \"phÃ¢n loáº¡i hÃ ng silver ipad air 4 cÃ³ kiá»ƒu dÃ¡ng má»›i tÆ°Æ¡ng tá»± iPad Pro 2020 nhÆ°ng cÃ³ kÃ­ch thÆ°á»›c nhá» hÆ¡n vÃ  dÃ y chá»‰ 6.1 mm, trá»ng lÆ°á»£ng Ä‘áº¡t 460 g dá»… dÃ ng mang theo bÃªn mÃ¬nh má»i lÃºc má»i nÆ¡i thiáº¿t káº¿ nÃ y giÃºp tÆ°Æ¡ng thÃ­ch vá»›i bÃ n phÃ­m Apple Smart Keyboard Folio Magic Keyboard cá»§a iPad Pro 11 inch vÃ  há»— trá»£ bÃºt Apple Pencil 2\"\r\n",
    "print(SentenceSpliter(sentence))\r\n",
    "print(SentenceCombiner(SentenceCorrector(SentenceSpliter(sentence))))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['phÃ¢n', 'loáº¡i', 'hÃ ng', 'silver', 'ipad', 'air', '4', 'cÃ³', 'kiá»ƒu', 'dÃ¡ng', 'má»›i', 'tÆ°Æ¡ng', 'tá»±', 'iPad', 'Pro', '2020', 'nhÆ°ng', 'cÃ³', 'kÃ­ch', 'thÆ°á»›c', 'nhá»', 'hÆ¡n', 'vÃ ', 'dÃ y', 'chá»‰', '6.1', 'mm,', 'trá»ng', 'lÆ°á»£ng', 'Ä‘áº¡t', '460', 'g', 'dá»…', 'dÃ ng', 'mang', 'theo', 'bÃªn', 'mÃ¬nh', 'má»i', 'lÃºc', 'má»i', 'nÆ¡i', 'thiáº¿t', 'káº¿', 'nÃ y', 'giÃºp', 'tÆ°Æ¡ng', 'thÃ­ch', 'vá»›i', 'bÃ n', 'phÃ­m', 'Apple', 'Smart', 'Keyboard', 'Folio', 'Magic', 'Keyboard', 'cá»§a', 'iPad', 'Pro', '11', 'inch', 'vÃ ', 'há»—', 'trá»£', 'bÃºt', 'Apple', 'Pencil']\n",
      "phÃ¢n hÃ ng cÃ³ kiá»ƒu dÃ¡ng má»›i tÆ°Æ¡ng tá»± nhÆ°ng cÃ³ kÃ­ch thÆ°á»›c nhá» hÆ¡n vÃ  dÃ y chá»‰ trá»ng lÆ°á»£ng Ä‘áº¡t dá»… dÃ ng mang theo bÃªn mÃ¬nh má»i lÃºc má»i nÆ¡i thiáº¿t káº¿ nÃ y giÃºp tÆ°Æ¡ng thÃ­ch vá»›i bÃ n phÃ­m cá»§a vÃ  há»— trá»£ bÃºt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Thá»© tá»± cháº¡y: 4\r\n",
    "\r\n",
    "import re\r\n",
    "import string\r\n",
    "\r\n",
    "#LÃ m sáº¡ch dá»¯ liá»‡u review\r\n",
    "def normalize_text(texts):\r\n",
    "    \r\n",
    "    #Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯\r\n",
    "    replace_list = {\r\n",
    "        'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',\r\n",
    "        'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ', 'uáº£': 'á»§a',\r\n",
    "        'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘','Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',\r\n",
    "        'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»','ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',\r\n",
    "        'eÌ‰': 'áº»', 'Ã k': u' Ã  ','aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯','Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',\r\n",
    "        #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words\r\n",
    "        ':)) ': ' hÃ i lÃ²ng ', ' shiper ': ' ngÆ°á»i_giao_hÃ ng ', ' shipper ': ' ngÆ°á»i_giao_hÃ ng ', ' giao hÃ ng ': ' giao_hÃ ng ' , ' mng ': ' má»i ngÆ°á»i ', ' tnao ': ' tháº¿ nÃ o ' , ' m.n ': ' má»i ngÆ°á»i' , ' tr ': ' trá»i ', ' nma ': ' nhÆ°ng mÃ  ', ' mn ': ' má»i ngÆ°á»i ', ' mk ': ' mÃ¬nh ' , ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',\r\n",
    "        'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',\r\n",
    "        ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',\r\n",
    "        'kg ': u' khÃ´ng ','not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '\"k ': u' khÃ´ng ',' kh ':u' khÃ´ng ','kÃ´':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '\"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',\r\n",
    "        'he he': ' thÃ­ch ','hehe': ' thÃ­ch ','hihi': ' thÃ­ch ', 'haha': ' thÃ­ch ', 'hjhj': ' thÃ­ch ',\r\n",
    "        ' lol ': ' tá»‡ ',' cc ': ' tá»‡ ','cute': u' dá»… thÆ°Æ¡ng ','huhu': ' tá»‡ ', ' vs ': u' vá»›i ', 'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',\r\n",
    "        ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',\r\n",
    "        'Ä‘c': u' Ä‘Æ°á»£c ','authentic': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' thÃ­ch ', 'store': u' cá»­a hÃ ng ',\r\n",
    "        'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ','god': u' tá»‘t ','wel done':' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',\r\n",
    "        'sáº¥u': u' xáº¥u ','gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': u' bÃ¬nh thÆ°á»ng ',\r\n",
    "        'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',\r\n",
    "        'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng','chat':' cháº¥t ', 'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡','fresh': ' tÆ°Æ¡i ','sad': ' tá»‡ ',\r\n",
    "        'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',\r\n",
    "        'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',\r\n",
    "        'cháº¥t lg': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u'bjo':u' bao giá» ',\r\n",
    "        'thik': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u'quáº£ ng ':u' quáº£ng  ',\r\n",
    "        'dep': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ','delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ',\r\n",
    "        'iu': u' yÃªu ','fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' thÃ­ch ',\r\n",
    "        ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', 'ib':u' nháº¯n tin ', 'rep':u' tráº£ lá»i ',u'fback':' feedback ','fedback':' feedback ',\r\n",
    "        #dÆ°á»›i 3* quy vá» 1*, trÃªn 3* quy vá» 5*\r\n",
    "        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\r\n",
    "        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\r\n",
    "        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}\r\n",
    "    \r\n",
    "    sents = []\r\n",
    "    cau = '';\r\n",
    "    count = 0\r\n",
    "    for text in texts:\r\n",
    "        count += 1\r\n",
    "        #print(text['Nhan xet'])\r\n",
    "        cau = text['Nhan xet']\r\n",
    "        #cau = normalize_text(cau)\r\n",
    "        sao = text['Sao']\r\n",
    "        #Sá»­a cÃ¡c icon, tá»« viáº¿t táº¯t\r\n",
    "        if cau == None:\r\n",
    "            cau = ''\r\n",
    "        for k, v in replace_list.items():\r\n",
    "                cau = cau.replace(k, v)\r\n",
    "        #print(cau)\r\n",
    "        #Remove cÃ¡c kÃ½ tá»± kÃ©o dÃ i: vd: Ä‘áº¹ppppppp\r\n",
    "        texts = cau.split()\r\n",
    "        len_text = len(texts)\r\n",
    "        for i in range(len_text):\r\n",
    "            cp_text = texts[i]\r\n",
    "        if cp_text in not_list: # Xá»­ lÃ½ váº¥n Ä‘á» phá»§ Ä‘á»‹nh (VD: Ã¡o nÃ y cháº³ng Ä‘áº¹p--> Ã¡o nÃ y notpos)\r\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\r\n",
    "\r\n",
    "            for j in range(numb_word):\r\n",
    "                if texts[i + j + 1] in pos_list:\r\n",
    "                    texts[i] = 'notpos'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "\r\n",
    "                if texts[i + j + 1] in nag_list:\r\n",
    "                    texts[i] = 'notnag'\r\n",
    "                    texts[i + j + 1] = ''\r\n",
    "        #else: #ThÃªm feature cho nhá»¯ng sentiment words (Ã¡o nÃ y Ä‘áº¹p--> Ã¡o nÃ y Ä‘áº¹p positive)\r\n",
    "            #if cp_text in pos_list:\r\n",
    "            #    texts.append('')\r\n",
    "            #if cp_text in nag_list:\r\n",
    "            #    texts.append('nagative')\r\n",
    "        \r\n",
    "        cau = u' '.join(texts)\r\n",
    "        cau = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), cau, flags=re.IGNORECASE)\r\n",
    "        preinput = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', cau)\r\n",
    "        pre = dict()\r\n",
    "        \r\n",
    "        #XÃ³a kÃ­ tá»± Ä‘áº·c biá»‡t\r\n",
    "        for i in preinput:\r\n",
    "            pre['Nhan xet'] = re.sub(r'[^a-zA-ZÃ€-á»¹\\s]', '', i, re.I|re.A).lower()\r\n",
    "            pre['Nhan xet'] = SentenceCombiner(SentenceCorrector(SentenceSpliter(i)))\r\n",
    "            pre['Sao'] = sao\r\n",
    "        sents.append(pre)\r\n",
    "        clear_output(wait=True)\r\n",
    "        print('Review', count , ':', pre['Nhan xet'])\r\n",
    "        print('.\\n.\\n.')\r\n",
    "    return sents\r\n",
    "\r\n",
    "def pre_process(raw_data):\r\n",
    "    data = normalize_text(raw_data)\r\n",
    "    #data.pop()\r\n",
    "    return data\r\n",
    "\r\n",
    "#BÆ°á»›c chuáº©n hÃ³a dá»¯ liá»‡u\r\n",
    "#for r in range(len(seg)):\r\n",
    "#    seg[r] = normalize_text(seg[r])\r\n",
    "seg = pre_process(data)\r\n",
    "#print(seg[:5]) #DÃ²ng nÃ y Ä‘á»ƒ xem cÃ¡c review Ä‘Ã£ qua tiá»n xá»­ lÃ½\r\n",
    "print('ÄÃ£ xá»­ lÃ½ xong cÃ¡c review.')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Review 149 : quÃ¡ cá»­a hÃ ng Ä‘Ã³ng gÃ³i ká»¹ cÃ³ Ä‘iá»u giao hÆ¡i nhiá»‡t dá»…\n",
      ".\n",
      ".\n",
      ".\n",
      "ÄÃ£ xá»­ lÃ½ xong cÃ¡c review.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Thá»© tá»± cháº¡y: 5\r\n",
    "#Tokenize cÃ¡c tá»« trong review\r\n",
    "from pyvi import ViTokenizer\r\n",
    "segs = []\r\n",
    "cnt = 1\r\n",
    "for token in seg:\r\n",
    "    sub_segs = []\r\n",
    "    clear_output(wait=True)\r\n",
    "    #print('Xá»­ lÃ½ review:', cnt, ':', token['Nhan xet'])\r\n",
    "    cnt += 1\r\n",
    "    doc = ViTokenizer.tokenize(str(token['Nhan xet']))\r\n",
    "    #print(pos_tag(str(sub))\r\n",
    "    sub_segs.append(str(doc))\r\n",
    "    segs.append(sub_segs)\r\n",
    "#print(segs[:5])\r\n",
    "print('HoÃ n thÃ nh tokenize.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HoÃ n thÃ nh tokenize.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#Thá»© tá»± cháº¡y: 6\r\n",
    "#Chá»©a cÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n lá»›p\r\n",
    "review_class = []\r\n",
    "\r\n",
    "\r\n",
    "#TÃ¡ch cÃ¢u:\r\n",
    "def sentence_segment_original(text):\r\n",
    "    sents = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', text)\r\n",
    "    return sents\r\n",
    "\r\n",
    "#Xá»­ lÃ½\r\n",
    "for rv in segs:\r\n",
    "    singleRVClass = []\r\n",
    "    y_pred = clf.predict(rv)\r\n",
    "    for i in y_pred:\r\n",
    "        singleRVClass.append(i)\r\n",
    "    review_class.append(singleRVClass)\r\n",
    "#print(review_class)\r\n",
    "\r\n",
    "#Tinh tong % tot / xau tren tat ca review\r\n",
    "def Calculate_Review_Score(pred):\r\n",
    "    score = []\r\n",
    "    for i in pred:\r\n",
    "        sum = 0\r\n",
    "        for j in i:\r\n",
    "            if j == 0:\r\n",
    "                sum += 1\r\n",
    "        score.append(sum/len(i))\r\n",
    "    #print(score)\r\n",
    "    for i in score:\r\n",
    "        sum += i\r\n",
    "    sum /= len(score)\r\n",
    "    return(sum)\r\n",
    "    #if sum < 0.3:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ ráº¥t tá»‡'\r\n",
    "    #elif sum < 0.4:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ tá»‡'\r\n",
    "    #elif sum <= 0.5:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ khÃ¡ tá»‡'\r\n",
    "    #elif sum < 0.7:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ khÃ¡ tá»‘t'\r\n",
    "    #elif sum < 0.85:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ tá»‘t'\r\n",
    "    #else:\r\n",
    "    #    return 'Sáº£n pháº©m Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ ráº¥t tá»‘t'\r\n",
    "\r\n",
    "Product_Score = Calculate_Review_Score(review_class)\r\n",
    "if Product_Score <= 0.5:\r\n",
    "    good_bad = 1\r\n",
    "else: good_bad = 0\r\n",
    "print('Äiá»ƒm Ä‘Ã¡nh giÃ¡ sáº£n pháº©m dá»±a trÃªn review (thang Ä‘iá»ƒm 10): ', Product_Score*10)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Äiá»ƒm Ä‘Ã¡nh giÃ¡ sáº£n pháº©m dá»±a trÃªn review (thang Ä‘iá»ƒm 10):  6.308724832214764\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Thá»© tá»± cháº¡y: 7\r\n",
    "\r\n",
    "### SECTION LOAI BO STOPWORDS ###\r\n",
    "from underthesea import pos_tag\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "#ThÃªm dá»¯ liá»‡u tá»« file\r\n",
    "file = open('./data/vietnamese_stopwords.txt','r', encoding = \"utf8\")\r\n",
    "stopwords = file.read()\r\n",
    "df = pd.read_csv('./data/600_adj_pairs.txt', delimiter=\"\\t\", sep=\" \", encoding = \"utf8\", index_col=\"Relation\")\r\n",
    "df.columns = ['Word1', 'Word2']\r\n",
    "SYN = [] #Danh sÃ¡ch tá»« Ä‘á»“ng nghÄ©a\r\n",
    "#print(df)\r\n",
    "SYN = df.loc[\"SYN\"]\r\n",
    "#print(SYN['Word1'])\r\n",
    "\r\n",
    "\r\n",
    "segg = segs.copy()\r\n",
    "#print(stopwords)\r\n",
    "stopword = re.split('(?<!^)\\s*[.\\n]+\\s*(?!$)', stopwords)\r\n",
    "i = x =0\r\n",
    "\r\n",
    "#Thay tháº¿ khoáº£ng trá»‘ng cá»§a tá»« trong bá»™ stopword báº±ng kÃ½ tá»± '_' Ä‘á»ƒ dá»… xá»­ lÃ½ hÆ¡n\r\n",
    "#stopwords = []\r\n",
    "for word in stopword:\r\n",
    "    stopword[x] = word.replace(' ', '_')\r\n",
    "    x+=1\r\n",
    "    #print(word)\r\n",
    "\r\n",
    "\r\n",
    "### LOAI BO STOPWORDS ###\r\n",
    "segss = []\r\n",
    "for a in segg:\r\n",
    "    tempseg = []\r\n",
    "    for b in a:\r\n",
    "        tempb = ''\r\n",
    "        for c in b:\r\n",
    "            #print(b)\r\n",
    "            if c == ' ':\r\n",
    "                #print(tempb)\r\n",
    "                if tempb not in stopword and tempb != '':\r\n",
    "                    tempseg.append(tempb)\r\n",
    "                if tempb == 'tá»‘t': print(tempseg)\r\n",
    "                tempb = ''\r\n",
    "            else:\r\n",
    "                tempb += c\r\n",
    "        #tempseg.append(tempb)\r\n",
    "    if tempseg != []:\r\n",
    "        segss.append(tempseg)\r\n",
    "    i += 1\r\n",
    "#print(segss[:5])\r\n",
    "\r\n",
    "#Duyá»‡t táº¥t cáº£ cÃ¡c tÃ­nh tá»« vÃ o báº£ng TABLE_ADJ\r\n",
    "TABLE_ADJ = []\r\n",
    "KiemTra = []\r\n",
    "Count_All_Adjs = 0\r\n",
    "count = 0\r\n",
    "for i in segss:\r\n",
    "    adj = dict()\r\n",
    "    cau = ''\r\n",
    "    for b in i:\r\n",
    "        cau += b + ' '\r\n",
    "    a = pos_tag(cau)\r\n",
    "    count += 1\r\n",
    "    #print(a[0][1])\r\n",
    "    for b in pos_tag(cau):\r\n",
    "        if b[1] == 'A':\r\n",
    "            if b[0] not in KiemTra:\r\n",
    "                adj['WORD'] = b[0]\r\n",
    "                KiemTra.append(b[0])\r\n",
    "                clear_output(wait=True)\r\n",
    "                print('XÃ©t cÃ¢u', count, ':', cau)\r\n",
    "                print('Loáº¡i tá»«: ', b[1], ', Tá»«: ', b[0])\r\n",
    "                #print(KiemTra)\r\n",
    "                print('Sá»‘ lÆ°á»£ng tÃ­nh tá»« Ä‘Ã£ xÃ©t:', Count_All_Adjs)\r\n",
    "                print('Sá»‘ lÆ°á»£ng tÃ­nh tá»« khÃ¡c nhau trong báº£ng: ', len(TABLE_ADJ))\r\n",
    "                similar_words_1 = SYN.loc[SYN['Word1'] == b[0]]\r\n",
    "                similar_words_2 = SYN.loc[SYN['Word2'] == b[0]]\r\n",
    "                adj['COUNT'] = 1\r\n",
    "                if similar_words_1.empty != True:\r\n",
    "                    adj['SIMILAR'] = similar_words_1['Word2'].unique()[0]\r\n",
    "                    adj['COUNT'] += 1\r\n",
    "                    KiemTra.append(adj['SIMILAR'])\r\n",
    "            \r\n",
    "                if similar_words_2.empty != True:\r\n",
    "                    adj['SIMILAR'] = similar_words_2['Word1'].unique()[0]\r\n",
    "                    adj['COUNT'] += 1\r\n",
    "                    KiemTra.append(adj['SIMILAR'])\r\n",
    "                    \r\n",
    "                else:\r\n",
    "                    adj['SIMILAR'] = ''\r\n",
    "                if adj not in TABLE_ADJ:\r\n",
    "                    TABLE_ADJ.append(adj)\r\n",
    "                    #print(TABLE_ADJ)\r\n",
    "            else:\r\n",
    "                for x in TABLE_ADJ:\r\n",
    "                    if x['WORD'] == b[0]:\r\n",
    "                        x['COUNT'] += 1\r\n",
    "                    elif x['SIMILAR'] == b[0]:\r\n",
    "                        x['COUNT'] += 1\r\n",
    "            Count_All_Adjs += 1\r\n",
    "print('Sá»‘ lÆ°á»£ng tÃ­nh tá»« Ä‘Ã£ xÃ©t xong: ', Count_All_Adjs)\r\n",
    "TABLE_ADJ.append(Count_All_Adjs)\r\n",
    "#print(TABLE_ADJ)\r\n",
    "#print(TABLE_ADJ[:5])\r\n",
    "#print(segss[:4])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "XÃ©t cÃ¢u 116 : trá»£ gá»­i hoÃ¡_Ä‘Æ¡n mua hÃ ng siÃªu Ä‘á»c nháº¯n thÃ¨m pháº£n phá»¥c_vá»¥ kÃ©m tráº£_lá»i \n",
      "Loáº¡i tá»«:  A , Tá»«:  kÃ©m\n",
      "Sá»‘ lÆ°á»£ng tÃ­nh tá»« Ä‘Ã£ xÃ©t: 80\n",
      "Sá»‘ lÆ°á»£ng tÃ­nh tá»« khÃ¡c nhau trong báº£ng:  19\n",
      "Sá»‘ lÆ°á»£ng tÃ­nh tá»« Ä‘Ã£ xÃ©t xong:  82\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#Thá»© tá»± cháº¡y: 8\r\n",
    "\r\n",
    "#Kiá»ƒm tra Ä‘á»™ tin tÆ°á»Ÿng cá»§a tá»«ng review dá»±a trÃªn báº£ng tÃ­nh tá»«:\r\n",
    "import numpy as np\r\n",
    "def Review_score(TatCaReviewDaXuLy, TABLE_ADJ, review, index, rating_star):\r\n",
    "    score = 0\r\n",
    "    constrain_2tt = 0\r\n",
    "    constrain_2tt_trung = 0\r\n",
    "    if abs(rating_star - TatCaReviewDaXuLy[index]['Sao']) <= 2:\r\n",
    "        score += 0.2\r\n",
    "    else: score += 0.1\r\n",
    "    if clf.predict([review])[0] == good_bad:\r\n",
    "        score += 0.1*good_bad\r\n",
    "    else:\r\n",
    "        score += 0.1*(1-good_bad)\r\n",
    "    for word in pos_tag(review):\r\n",
    "        if word[1] == 'N':\r\n",
    "            constrain_2tt += 1\r\n",
    "        if word[1] == 'A':\r\n",
    "            constrain_2tt += 1\r\n",
    "            if constrain_2tt == 2:\r\n",
    "                score += 0.1\r\n",
    "            for x in TABLE_ADJ[:-1]:\r\n",
    "                if x['WORD'] == word[0]:\r\n",
    "                    constrain_2tt_trung += 1\r\n",
    "                    if constrain_2tt_trung == 2:\r\n",
    "                        score += 0.1\r\n",
    "                    score += x['COUNT']/TABLE_ADJ[-1]*0.5\r\n",
    "    return score\r\n",
    "mini = 1\r\n",
    "maxi = 0\r\n",
    "All_review_score = []\r\n",
    "idx = 0\r\n",
    "idx_min = idx_max = 0\r\n",
    "cnt = 1\r\n",
    "for i in segss:\r\n",
    "    cau = ''\r\n",
    "    for b in i:\r\n",
    "        cau += b + ' '\r\n",
    "        score = Review_score(seg, TABLE_ADJ, cau, idx, rating_star)\r\n",
    "        if score > 1:\r\n",
    "            score = 1\r\n",
    "        if score < 0: score = 0\r\n",
    "    if score <= mini: \r\n",
    "        mini = score\r\n",
    "        idx_min = cnt - 1\r\n",
    "    if score >= maxi and score < 0.9: \r\n",
    "        maxi = score\r\n",
    "        idx_max = cnt - 1\r\n",
    "    clear_output(wait=True)\r\n",
    "    print('Sao trung bÃ¬nh:', rating_star)\r\n",
    "    print('TÃ­nh Ä‘iá»ƒm cho review', cnt, ':', score)\r\n",
    "    print('Äiá»ƒm tháº¥p nháº¥t:', mini, '\\nReview:', data[idx_min]['Nhan xet'], '\\nSao:', data[idx_min]['Sao'], '\\nÄiá»ƒm cao nháº¥t:', maxi, '\\nReview:', data[idx_max]['Nhan xet'], '\\nSao:', data[idx_max]['Sao'])\r\n",
    "    cnt += 1\r\n",
    "    All_review_score.append(score)\r\n",
    "    idx += 1\r\n",
    "print(All_review_score[:30])\r\n",
    "#print(All_review_score[int(input('Nháº­p thá»© tá»± cÃ¢u:'))])\r\n",
    "np_review = np.array(All_review_score)\r\n",
    "#print(All_review_score)\r\n",
    "#print(seg[:6])\r\n",
    "idx = np.argpartition(np_review, -4)[-4:]\r\n",
    "#print(idx)\r\n",
    "print('CÃ¡c review cÃ³ Ä‘iá»ƒm cao nháº¥t:')\r\n",
    "for i in idx:\r\n",
    "    print(data[i]['Nhan xet'], ' score: ', All_review_score[i])\r\n",
    "    #print('Score:',All_review_score[i])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sao trung bÃ¬nh: 4.879432624113475\n",
      "TÃ­nh Ä‘iá»ƒm cho review 121 : 0.42439024390243907\n",
      "Äiá»ƒm tháº¥p nháº¥t: 0.2 \n",
      "Review: Sáº£n pháº©m tá»‘t. Giao hÃ ng nhanh! Mua Ä‘c flash sale nÃªn giÃ¡ khÃ¡ ráº»! Sáº½ á»§ng há»™ cÃ¡c sáº£n pháº©m khÃ¡c cho shop! \n",
      "Sao: 5 \n",
      "Äiá»ƒm cao nháº¥t: 0.5158536585365854 \n",
      "Review: bá»± hÆ¡n mÃ¬nh tÆ°á»Ÿng, xÃ¡c nháº­n hÆ¡i lÃ¢u nhÆ°ng xá»©ng Ä‘Ã¡ng Ä‘á»ƒ chá» Ä‘á»£i nhe \n",
      "Sao: 5\n",
      "[0.30000000000000004, 0.3243902439024391, 0.2, 0.2, 0.2, 0.31829268292682933, 0.31219512195121957, 0.30000000000000004, 0.30000000000000004, 0.20609756097560977, 0.24268292682926831, 0.30000000000000004, 0.30000000000000004, 0.42439024390243907, 0.31219512195121957, 0.2, 0.2, 0.2, 0.21829268292682927, 0.30000000000000004, 0.2304878048780488, 0.22439024390243903, 0.26707317073170733, 0.30000000000000004, 0.2, 0.31829268292682933, 0.44268292682926835, 0.39146341463414636, 0.2, 0.2]\n",
      "CÃ¡c review cÃ³ Ä‘iá»ƒm cao nháº¥t:\n",
      "Giao hÃ ng siÃªu tá»‘c chá»‰ sau 3 ngÃ y khÃ´ng tÃ­nh ngÃ y Ä‘áº·t Ä‘Ã£ nháº­n Ä‘Æ°á»£c hÃ ng dÃ¹ Ä‘ang khÃ³ khÄƒn mÃ¹a dá»‹ch. Má»i thá»© Ä‘á»u Ä‘áº¹p vÃ  nguyÃªn váº¹n.  score:  0.44268292682926835\n",
      "Sáº£n pháº©m chÃ­nh hÃ£ng tá»‘t, chua active, minh mua sales 8.8 nÃªn chi 13,2tr, gia rÃ¢t tÃ´t, giao cung nhanh  score:  0.4487804878048781\n",
      "Äá»£i hÃ ng ráº¥t lÃ¢u mn áº¡. Huhu. Máº¥t 5 ngÃ y Ã­. CÃ²n ipad chuáº©n luÃ´n nhÃ©. GÃ³i cx ráº¥t cáº©n tháº­n. Box to láº¯m khÃ¡ ok nhÃ©  score:  0.4548780487804878\n",
      "bá»± hÆ¡n mÃ¬nh tÆ°á»Ÿng, xÃ¡c nháº­n hÆ¡i lÃ¢u nhÆ°ng xá»©ng Ä‘Ã¡ng Ä‘á»ƒ chá» Ä‘á»£i nhe  score:  0.5158536585365854\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}